\documentclass[a4paper,11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
\usepackage[pdftex]{graphicx}
\usepackage{fouriernc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{booktabs}	
\usepackage{multirow}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{titlesec} 
\usepackage{parskip}
\usepackage{url}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{sectsty}
\partfont{\large}

\usepackage{etoolbox}
\patchcmd{\abstract}{\titlepage}{\clearpage}{}{}
\patchcmd{\andabstract}{\endtitlepage}{\clearpage}{}{}


%\addtolength{\oddsidemargin}{-.5in}
%\addtolength{\evensidemargin}{-.5in}
%\addtolength{\textwidth}{1.75in}

%\addtolength{\topmargin}{-.875in}
%\addtolength{\textheight}{1.75in}

\newcommand{\toclesssection}[1]{\section{#1}\addtocounter{section}{1}}


\usepackage[acronym]{glossaries}
\makeglossaries
% abbreviations:
\newacronym{hpc}{HPC}{High Performance Computing}
\newacronym{gcm}{GCM}{Global Circulation Model}
\newacronym{fft}{FFT}{Fast Fourier Transform}
\newacronym{fftw}{FFTW}{Fastest Fourier Transform in the West}
\newacronym{nag}{NAG}{Numerical Algorithms Group}
\newacronym{mpi}{MPI}{Message Passing Interface}
\newacronym{mpp}{MPP}{Massively Parallel Module}
\newacronym{ulp}{ULP}{Units of Last Place}
\newacronym{fms}{FMS}{Flexible Modelling System}
\newacronym{dft}{DFT}{Discrete Fourier Transform}
\newacronym{idft}{IDFT}{Inverse Discrete Fourier Transform}
\newacronym{sve}{SVE}{Scalable Vector Extensions}
\newacronym{bcp3}{BCP3}{BlueCrystal phase 3}
\newacronym{bcp4}{BCP4}{BlueCrystal phase 4}
\newacronym{bp}{BP}{BluePebble}
\newacronym{avx}{AVX}{Advanced Vector Extensions}
\newacronym{flops}{FLOPS}{Floating Point Operations}
\newacronym{epsrc}{EPSRC}{Engineering and Physical Sciences Research Council}
\newacronym{acrc}{ACRC}{Advanced Computing Research Centre}
\newacronym{hpl}{HPLinpack}{High Performance Linpack}
\newacronym{nerc}{NERC}{National Environmental Research Council}
\newacronym{pbs}{PBS}{Portable Batch System}
\newacronym{icc}{ICC}{Intel Compiler Collection}
\newacronym{gcc}{GCC}{Gnu Compiler Collection}
\newacronym{cce}{CCE}{Cray Compiling Environment}









% Fancy pages and chapters
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RE,LO]{\leftmark}
\fancyfoot[C]{\thepage}
\titleformat{\chapter}[display]
  {\bfseries\Large}
  {\filright\MakeUppercase{\chaptertitlename} \Huge\thechapter}
  {1ex}
  {\titlerule\vspace{1ex}\filleft}
  [\vspace{1ex}\titlerule]
 \titlespacing*{\chapter}{0pt}{-30pt}{25pt}
 
%\titlespacing{\section}{10pt}{\parskip}{-\parskip}
%\titlespacing{\subsection}{15pt}{\parskip}{-\parskip}
%\titlespacing{\subsubsection}{0pt}{\parskip}{-\parskip}
  

 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
  
  
  
\title{Porting and optimising the Isca climate model on Intel and Arm processors}
\author{George Lancaster}
\begin{document}



\pagenumbering{roman}
\includepdf[pages=-]{img/frontmatter_64327.pdf}
\tableofcontents{}
\listoffigures
\listoftables
\printglossary[type=\acronymtype,title=List of Abbreviations]

\chapter*{Abstract}
   This research project explores...
   \par
This project provides ports of the Isca climate model to four supercomputers, making over 26,000 cores available for climate research at the University of Bristol. The limitations of the model have been identified, and a  Additionally, it provides a valuable benchmark of the ThunderX2 processor in a production environment, contributing to current research.


\cleardoublepage%%%very important

\pagenumbering{arabic}
\part{Introduction and Background}
\chapter{Introduction}
\label{chap:intro}
\section{High Performance Computing}
\label{sec:sec01}
\begin{itemize}
	\item What is the problem?
	\item How have I solved it?
\end{itemize}
\gls{hpc} is an interdisciplinary field within computer science that uses large distributed supercomputers to solve complex problems across many scientific domains. In academia, HPC is synonymous with scientific computing and has applications including drug discovery, artificial intelligence and the simulation of the natural world. 
\par
Supercomputers are usually comprised of many compute nodes, each containing a number of high performance server microprocessors. Recent improvements to computational hardware have been notoriously difficult to utilise, and many scientific codes remain unoptimised throughout many years of service. This trend means that there can be poor `\textit{price per performance}' of hardware, as the latest features available to modern processors are underutilised. 
\par
Climate models are an important tool for understanding the global atmospheric behaviour of Earth and other celestial bodies. They provide a medium for the reproduction of past, present and future meteorological events, and offer insight into previously unobservable phenomena. However, complex simulations can take thousands of hours to return substantial results, limiting the speed of research.
\par
Climate models have many parameters governing simulated physical processes. Many of these parameters are well defined by observation, however there are equally many that are idealised, and do not correspond to processes found in the real world. Selecting values for such parameters is non-trivial, and is usually achieved using a brute-force approach known as a ‘perturbed physics ensemble’, which involves running many simulations using a range of parameter configurations. This strains supercomputer resources, as jobs are typically submitted in large batches, with each job taking many hours to complete.
%One of the aims of HPC is to fully utilise the advanced processing power of modern hardware.

\section{Aims and Objectives}
This research project aims to present a comprehensive performance analysis of the Isca climate model on both Intel and Arm processors. The model must be ported to four HPC systems, and optimised with the goal of reducing the models runtime. To meet this aim, the following objectives have been identified:

\begin{description}
	\item[Port Isca to three new HPC systems] Isca must be ported from the \gls{bcp4} supercomputer to three other HPC systems: \gls{bcp3}, \gls{bp} and Isambard. \gls{bcp3}, \gls{bcp4} and \gls{bp} are based on the Intel x86-64 architecture, and Isambard is based on Arm's ARMv8 instruction set architecture. Isca is dependant on many libraries that are not yet to be available on the Arm machine. Identifying and porting these libraries comprised a significant part of the project.
	\item[Characterisation of the Isca code] Isca must be benchmarked and profiled using a variety of performance analysis tools to identify the code’s limitations. The resulting data must be used to plan at least two performance optimisations. Additionally, runtimes on each system will be measured to find how the total program runtime varies as a function of cell resolution and number of processor cores. 
	
	\item[Optimisation of Isca on each system] All identified performance optimisations must be implemented to a high standard. All code modifications must follow the same style and naming conventions as found in the rest of the codebase.
	
	\item[Analysis of Optimisations] To ensure that the optimisations improve the model’s performance, the optimised code must be recharacterised and compared to the unoptimised model. To be deemed successful, an optimisation must deliver a significant improvement to performance and must generate the same output as the unoptimised code. This will verify that the application logic is unchanged. It is also important to measure the performance portability of the optimisations, as a performance improvement on one machine may not carry over to another.
	
\end{description}

\section{Contributions}
The work presented in this thesis makes the following contributions to the Isca codebase, and the wider area of high-performance computing:
\begin{description}
	\item[Provision of additional compute resources] Prior to this research project, University of Bristol researchers could only access Isca on \gls{bcp4}, one of five supercomputers available to the university. By providing ports of Isca to other systems, over 14,000 additional cores have be made available for climate research. Not only has this eased congestion on \gls{bcp4}, but it also makes Isca more accessible to other research groups outside of the University of Bristol. Additionally, the meteorological research group at the University of Bristol has purchased a dedicated £10,000 compute node for the BluePebble supercomputer as a direct result of the work carried out in this research project. 

	\item[Comprehensive performance analysis] A scaling study has been performed, which shows how the total program runtime varies as a function of cell resolution and number of processor cores. This is important for researchers as it allows them to make an informed decision when selecting the number of cores to run different model resolutions. Additionally, a number of performance bottlenecks have been identified, which adds to the understanding of the code. 
	
	\item[Optimisation of legacy code for modern hardware] This research project demonstrates that Isca does not utilise many of the new hardware features available to modern processors. Specifically, there are many loops found within a bespoke \gls{fft} do not make use of vector instructions. As a result of this observation, this \gls{fft} has been replaced with a call to the \gls{fftw} library, producing a code speedup of up to 1.17$\times$ the original implementation.
	
	\item[Increase speed of research] By halving the default precision of floating point numbers, Isca can better utilise vector registers. This presents a performance speedup of 1.69$\times$ the original code, and a speedup of 1.78$\times$ when used in conjunction with \gls{fftw}. 
	
	\item[Contribution to hardware development] Recent developments in consumer mobile hardware have resulted in a new generation of HPC-optimised Arm processors. This research project provides a comparison of these new processors and the current state-of-the-art Intel processors. This is vital to reduce the cost of components and to drive further innovation. This study is a continuation of previous work by other authors, and provides insight into the performance of the hardware on a production scientific code (citation).
	
	\item[Contribution to cluster development] The BluePebble cluster was still in its Beta phase of development throughout this research project. All results collected on this machine have been used to influence important design decisions, including the default stack-size limit and default memory limit for jobs submitted using the PBS job scheduler. Additionally, all dependencies required by Isca have been installed as modules using the build configurations defined by this research project, and these are freely available to use by other users of the systems. 
	
	\item[Contribution to the Isca codebase] All modifications made to the Isca codebase as a result of the work carried out in this project have been integrated back into the public Github repository in a series of pull-requests (reference). 
	
	\item[Documentation and support to researchers] Documentation has been written to help researchers compile and run Isca on different machines (TODO)
	
\end{description}


\section{Heading}
The work presented in this thesis was carried out over the the summer of 2019 alongside the University of Bristol HPC internship program. 




% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
% Chapter 2 - Background - - - - - - - - - - - - - - - 
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\chapter{A background on climate modelling}
\label{chap:background}
Software development for scientific applications is a multidisciplinary task, and requires knowledge of both computer programming and the scientific domain for which the software is being developed. Although a comprehensive understanding of the mathematics used to simulate climate is not necessary to understand the work presented in this thesis, some domain knowledge is required. This chapter presents a summary of climate modelling codes, and provides an extensive overview of the workings of Isca.

\section{The Isca climate model}
Isca is an open-source framework for the modelling of the global circulation of planetary atmospheres. It was developed over four years by the climatology research group at the University of Exeter, with version 1 of the model released in 2017 \cite{vallis2018isca}. The development of Isca was funded by the Natural Environment Research Council, the \gls{epsrc} and the UK Met Office \cite{vallis2018isca}.
\par
The main goal of Isca is to deliver a user-configurable climate model, that allows for the simulation of both simple and complex scenarios, including those vastly different from Earth. The model has been used to provide evidence for numerous peer-reviewed publications, including the study of monsoons, tidally-locked planets and variations in the seasons  \cite{penn2017thermal, thomson2018atmospheric, geen2018regime}.
 \par
 The model itself spans over 260,000 significant lines of code, and is composed primarily of Fortran with some calls to ANSI C, and a Python interface for usability. Isca can be compiled and run on any system with NetCDF and MPI libraries, although a supercomputer is required for anything more than simple experimentation \cite{vallis2018isca}. 
 \par
Because of its growing use as an academic research tool, it is of the utmost importance that the code is portable to a wide variety of computer architectures, and maintains a degree of performance portability. This will allow for the model to be used for research at other institutions and will drive future development, as Isca now has a well-established global user base. .

\subsection{Global Circulation Model (GCM)}
Although Isca is a new model, much of the code responsible for atmospheric simulation has been adapted from the twenty-one-year-old \gls{fms}, on which many modern climate models have been developed \cite{balaji2002fms, donner2011dynamical, farneti2009intermediate}. The FMS is a \gls{gcm}, and handles aspects of simulation including parallelisation, input and output, data exchange between model grids and the orchestration of time stepping \cite{gfdl2019fms}.
\par
GCMs simulate the changes in global climate behaviour over time using the set of primitive dynamical equations of motion and state, first described by Vilhelm Bjerknes in the early 20th century \cite{bjerknes1910dynamic,edwards2011history, godske1957dynamic}. These equations include the hydrodynamic state equation, mass conservation, and thermal energy equations, which govern the distribution of energy in the atmosphere \cite{vallis2018isca,edwards2011history}. In theory, these equations are applied in a continuous domain on the whole real line, however this is not possible to do in simulation due to the restrictions imposed by finite memory resources. To bypass this issue, \gls{gcm}s decompose the problem domain using a grid-point or spectral representation.

\subsection{Domain decomposition}
\label{sec:spectral-domain}
Grid-point models discretely represent data,  decomposing the problem domain into a three-dimensional structured grid, on which the primitive dynamical equations are applied at each time step of the simulation. Structured grid codes often have high spatial locality, with interactions between cells limited to adjacent neighbours only. This property means that they tend to be highly scalable, due to various spatial decomposition methods that utilise distributed machines effectively. 
\begin{figure}[htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{img/sphere-decomp.pdf}
  \caption{Grid-point model decomposition}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{img/waves.png}
  \caption{Spectral model decomposition}
  \label{fig:sub2}
\end{subfigure}
\caption[Grid-point and spectral domain decomposition]{A simplified example of a grid-point and spectral model projected onto the sphere. This example uses just 3 waves, which implies a truncation of T3. Most spectral models use a resolution in excess of 21 waves (T21). }
\label{fig:test}
\end{figure}
\par
Spectral models represent the spatial variations of atmospheric variables as a finite series of waves at various wavelengths, whereby each wave represents the coefficients of a known function. They are typically used for global climate modelling rather than regional weather prediction as wave functions and spherical harmonics operate over a spherical domain. Because of this, all waves must be periodic so that they wrap around the sphere with the start and finish point using the same value. This places some restrictions on the types of algorithms that can be used, and can add an additional overhead to compute costs as the model must convert into a spatial representation for analysis. 
\par
Calculating the equations of motion requires solving many partial derivatives in space. Partial derivatives of waves are calculated by summing the derivatives of each basis function, providing an exact result. In contrast, grid-point models must solve partial derivatives by finite differences, and therefore require a higher resolution to provide a comparable degree of accuracy \cite{bart1998coordinate}. 

\subsubsection{Resolution} 
The spatial resolution of a climate model describes the variation in the total amount of data that is used for a given problem size. The resolution of a grid-point model describes the number of grid cells that the model operates over. A higher cell resolution implies a greater number of cells contained within a grid. Spectral models vary their resolution using a truncation, which refers to the number of waves used to define atmospheric variables. In both cases, there is a trade-off between resolution and model runtime whereby higher resolutions generally result in longer runtimes, but more accurate results. 
\par
Climate models can also vary a temporal resolution, which refers to amount of model time that passes in the simulation between calculations. Similarly to spatial resolution, the computational intensity of the simulation is influenced by the granularity of the temporal resolution. Smaller time steps more accurately represent continuous time, but result in longer runtimes.
\par
Both grid-point and spectral models are usually classified as a strong scaling problem, for which the solution time varies with the number of processors for a fixed problem size [9]. This implies that the runtime decreases as the number of processor cores increases, however this is not always the case and is dependant on the problem domain.


\subsection{Fast Fourier Transform (FFT)}
The Isca modeluses both grid-point and spectral methods for domain decomposition. A grid-point representation is used for time-stepping, and the physics simulation is applied in the spherical and frequency domains. To convert between these two states, a \gls{fft} is used to compute the \gls{dft} of the grid-point representation, and the \gls{idft} of the spectral representation. Although the cost of doing this transformation can be relatively high, it often results in a net computational saving, and can produce more accurate data at lower resolutions (citation).
\par
The \gls{fft} algorithm is found across many different scientific domains, and as such writing optimised \gls{fft} code is a research topic in and of itself. There are multiple highly optimised \gls{fft} libraries available, and there are many different approaches to applying the algorithm, each with their own benefits and drawbacks. Formally the \gls{dft} transforms a sequence of $N$ complex numbers $\{x_{n}\} = \{ x_{0}, \: x_{1}, \: \ldots, \: x_{N-1} \}$ into another sequence of complex numbers $\{X_{k}\} = \{X_{0}, \: X_{1},  \: \ldots, \: X_{N-1} \}$, defined in Equation \ref{equ:dft}.

\begin{equation}
\label{equ:dft}
\mathcal{F}(x) = X_{k} = \sum _ { n = 0 } ^ { N - 1 } x _ { n } \cdot [ \cos ( 2 \pi k n / N ) - i \cdot \sin ( 2 \pi k n / N ) ]
\end{equation}
The \gls{dft} is invertible, which means that any complex vector whereby $N > 0$ has both a \gls{dft} and \gls{idft} of the same form as the original vector. The \gls{idft} is given in Equation \ref{equ:idft}.
\begin{equation}
\label{equ:idft}
I\mathcal{F}(x) = x _ { n } = \frac { 1 } { N } \sum _ { k = 0 } ^ { N - 1 } X _ { k } \cdot e ^ { i 2 \pi k n / N }
\end{equation}
\par

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% Chapter - Porting - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Software architecture}
The Isca codebase is vast, being composed of over 290 Fortran90 source files. This is far too much code to review for a project of this scope, however the following section provides a gentle introduction to the software architecture of the model.
\subsection{General overview}
Isca is compiled and run using its own Python library, which is used to populate various Bash scripts, Fortran namelists and other miscellaneous files with data entered into multiple dictionaries in the Python code. This was a design decision based on the usability of Python in comparison to the underlying Fortran model, and allows for a lower barrier to entry in terms of technical ability for climate researchers \cite{vallis2018isca}. 
\par
Compiling the model using the Python library produces a single executable that is repeatedly run for a number of iterations defined in a Python script. Typically, each iteration lasts for approximately one model month, usually simplified to 30 model days. When run in parallel the diagnostic output is distributed, which means that each processor writes its own files. Upon completion, the data generated by the previous month's simulation is combined into a single file, and is used as an input to the following month. This process is summarised by a flowchart in Figure \ref{fig:flowchart}. The large number of Python and Bash scripts used to create directories, and populate and move supporting files means that the executable cannot be run alone.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.75\textwidth]{img/isca_flow.pdf}
\caption{Flowchart that illustrates the program flow of Isca, when run using its Python library.}
\label{fig:flowchart}
\end{center}
\end{figure}
\par
\par
The executable itself follows an `atmosphere integration loop', whereby the state of the atmosphere is computed for a predefined number of timesteps. Isca is modular, which means that the atmosphere can be simulated using a wide range of different techniques and algorithms at varying degrees of complexity and realism, however the most basic atmosphere integration loop is visualised as pseudocode in Listing \ref{code:atmos-integration}.

\begin{lstlisting}[language=Fortran,label={code:atmos-integration},caption={Pseudocode for the atmospheric integration loop found in Isca.}]
Time_next = Time + Time_step

if(idealized_moist_model) then
   call idealized_moist_phys(...)
else
   call hs_forcing(...)
endif

call spectral_dynamics(Time, ...)

if(dry_model) then
  call compute_pressures_and_heights(x, z, ...)
else
  call compute_pressures_and_heights(x, y, ...)
endif

call spectral_diagnostics(Time_next, ...)

previous = current
current  = future
\end{lstlisting}
\par
Of greatest interest is the \texttt{spectral\_dynamics} subroutine, which comprises around 95\% of the wallclock runtime of any given simulation. This subroutine contains the code for calculating the atmospheric variables, which involves  communication between processors and a number of FFTs. 
\par
Isca's spectral model decomposes the horizontal grid into latitude bands, with each band assigned to a processor. When only two processors are used, the grid is split into Southern and Northern Hemispheres \cite{isca2019github}. This method of domain decomposition implies that atmospheric variables at the edge cases of each latitudinal band (halo points) must be exchanged with other processors in a process known as a synchronised halo exchange. This allows for parallelism in the Isca model at the cost of an additional overhead incurred by the communication itself. The halo exchange simply interrupts the computational flow of the program, and allows for the exchange of the halo points before the simulation can resume. Figure \ref{fig:halo-exchange} shows a simplified communication pattern similar to that found in Isca. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/comm_pattern.pdf}
\caption{Simple communication pattern; sections of compute are are interrupted by calls to a halo exchange.}
\label{fig:halo-exchange}
\end{center}
\end{figure}


\subsection{Dependencies}
\subsection{Fortran Libraries}
Isca relies on MPI for interprocess communication, and NetCDF for data storage. These are two technologies that are commonly used in high performance computing and offer interfaces for both the ANSI C and Fortran programming languages.

\begin{description}
	\item[MPI] \gls{mpi} is a standardised, portable interface for interprocess communication that allows for direct data transfer between processors without relying on shared memory. The MPI standard has been implemented by numerous companies and organisations, but the most commonly used are OpenMPI, MVAPICH, MPICH, and Intel MPI. All MPI implementations provide the same function calls and interfaces, and can therefore be used interchangeably. 
	
	\item[NetCDF] Network Common Data Format (NetCDF) is a platform independent binary file type that is commonly used to store and analyse scientific data. NetCDF binary files are self-describing, meaning that they contain the all the necessary information to interpret the data they store. This makes NetCDF files highly portable as a file written on one computer can be read by another without context or specialist tools, aside from the NetCDF library itself. If compiled using an MPI library, NetCDF can provide parallel IO. NetCDF itself is dependant on the HDF5 and zlib libraries, which are used for storage and data compression respectively. When compiling the NetCDF library or any program that uses it, the same compiler must be used to compile HDF5, zlib, NetCDF and the program itself. One of the advantages of NetCDF is that there are many programs available to visualise the data they store. Figure  \ref{fig:netcdf} show an example of a NetCDF file produced by Isca. 
\end{description}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{img/flux_t_in_atmos_daily.png}
\caption[Visual output from the Isca model]{Visualisation of an output of the Grey-Mars configuration, showing the amount of heat transferred per unit area per unit time to the Martian surface after 690 days. This visualisation was generated using the Panoply NetCDF data viewer tool (citation).}
\label{fig:netcdf}
\end{center}
\end{figure}


\subsection{Python Libraries}
Due to the simplicity of the language, Isca uses a Python interface to create and run different model configurations. To do this, it uses a number of popular Python libraries that are commonly available on many different platforms.

\begin{description}
	\item[Numpy] Numpy is a mathematics library for Python that allows for the manipulation of N-dimensional arrays.
	
	\item[sh] A full-featured subprocess replacement for Python. It allows for Bash commands to be issued from Python code \url{https://amoffat.github.io/sh/}. 
	
	\item[Jinja2] Jinja2 is a templating language for Python that is typically used in web design. It has been used in Isca to populate a number of Bash script templates with data defined in a series of Python dictionaries \url{http://jinja.pocoo.org/docs/2.10/}. 
	
	\item[f90nml] A Python module and command line tool for reading, writing and modifying Fortran namelist files \url{https://pypi.org/project/f90nml/}. 
	
\end{description}








\chapter{A background on HPC hardware and parallel processing}
There are many different techniques and processor designs that allow for a program to be run in parallel. This chapter presents a brief but thorough overview of some that have been used throughout this research project. 

\section{Parallel processing}
To allow for programs to be run in parallel, there are numerous different techniques that can be used. In order to improve the performance of a parallel code, understanding of these techniques are essential.

\subsection{Flynns Taxonomy}
Flynns taxonomy is a classification of parallel computing architectures first proposed by Michael J Flynn in 1966. Flynns taxonomy defines four unambiguous terms to describe the relationship between data and the technique by which it is processed. The entirety of Flynns taxonomy is visualised in Figure \ref{fig:flynns}, however the following bullet list provides further details of the architectures it describes.

\begin{description}
	\item[Single Instruction Single Data (SISD)] refers to the most basic type of processing; whereby a single instruction is applied to a single data item stored in memory. Code that uses this processing type is often referred to as scalar or serial. 
	
	\item[Single Instruction Multiple Data (SIMD)] allows for a single instruction to be applied to multiple data items stored in a contiguous piece of memory. To gain the largest performance benefit from SIMD operations, the multiple data items must be read using a single instruction, and then the same operation must be applied to all items. SIMD processing is often referred to as vectorisation, as the data is processed as a one-dimensional vector. 
	
	\item[Multiple Instruction Single Data (MISD)] is a rarely used processing technique that applies different operations on identical data. Rather than improving the performance of a program, it is often used for mission critical computations where there is no room for error. 
	
	\item[Multiple Instruction Multiple Data (MIMD)] is currently the most commonly used parallel processing technique. It describes a machine that contains many asynchronous processors that function independently, and as such, most modern processors can be categorised as MIMD machines.
\end{description}
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/flynns.pdf}
\caption[Flynns taxonomy]{Flynns taxonomy; instructions are applied to data by various processing elements (PE) in different ways. }
\label{fig:flynns}
\end{center}
\end{figure}
\par
Isca, like most scientific codes, uses the SIMD and MIMD approaches to parallelism. As discussed in Section \ref{blah}, the model uses the MPI library to split the domain into latitude bands.

\subsection{Instruction-level parallelism (SIMD)}
From the 1970's to the early 1990's, high performance machines relied heavily on instruction level vector operations to compute in parallel \cite{6877473}. These machines used vector processors, and performed operations on one-dimensional arrays of data, rather than single data items using a SIMD processor architecture \cite{conte2000long}. Many scientific codes from this time were written with this architecture in mind, and it is likely that it influenced the design and implementation of the FMS. 
\par
Instruction level parallelism using SIMD is becoming popular once again through the introduction of Intel's \gls{avx}. Intel introduced AVX in 2011's Sandy Bridge architecture, AVX-2 in 2013's Haswell architecture and AVX-512 in 2016's Skylake architecture \cite{chris2011avx, intel2019avx}. AVX-2 increased the width of some vector registers to 256 bits, allowing for SIMD operations on four, 64-bit elements per clock cycle. In comparison, the AVX-512 instruction set increased vector register width to 512 bits, allowing for SIMD operations on eight, 64-bit elements per clock cycle, double that of AVX-2 \cite{chris2011avx,xeon2680v4}.
\par
Although AVX-512 has a higher throughput of operations per clock-cycle, using wider vector registers results in greater power consumption, which in turn causes the processor to generate more heat. In order to maintain a suitable temperature, the processor will usually decrease its clock speed for the duration of the loop using the AVX-512 registers, often resulting in no overall performance gain over AVX-2.


\subsection{Message passing}
To enable communication between nodes, processors can explicitly communicate with each other using a message passing library. Most notably the MPI standard

Isca uses MPI as in 1998, compute nodes would only have a few processors per node. 

\subsection{Hardware}
At the time the FMS was developed, many scientific codes were limited by computational power \cite{mccalpin1995memory}. It is therefore likely that the FMS was optimised for the compute-bound systems of the time. 

\section{HPC clusters}
Throughout the course of this research project, Isca has been ported to and run on four different high-performance supercomputers. This section discusses these machines, and the features of their respective processor architectures. A full breakdown of the most important hardware features can be found in Table \ref{tbl:hardware}. \gls{bcp3}, \gls{bcp4} and \gls{bp} are all based on the well-established line of x86-64 Intel Xeon processors and Isambard is based on the ARM-v8 Cavium ThunderX2 processor.
\begin{table*}[!ht]
\centering
\caption[Hardware specifications of the target HPC systems]{Hardware specifications of the target HPC systems. }
\begin{tabular}{@{\extracolsep{4pt}}ccccc}
\toprule 
\multirow{2}{*}{\textbf{Attribute}} & \multicolumn{3}{c}{\textbf{Intel Xeon (x86-64)}} & \multicolumn{1}{c}{\textbf{ARMv8}}\\

				\cmidrule(lr){2-4} \cmidrule(lr){5-5}

 				& {\textbf{BC3}} 	& {\textbf{BC4}} 		& {\textbf{BP}} 		& {\textbf{Isambard}} \\
\midrule
%Machine		   	& blank		    	&blank				&					& Cray XC50 	\\
Processor	   		& E5-2670 v1 		& E5-2680 v4			& Gold 5120			& ThunderX2	\\
Codename            	& Sandy Bridge	        & Broadwell         		& Skylake       		   	& ThunderX2   	\\
Instruction set		& AVX			& AVX-2				& AVX-512			& NEON		\\
Clock Speed	   	& 2.6 GHz		    	& 2.4 GHz				& 2.2 GHz				&  2.1 GHz 	\\
Cores / Node		& $2\times8$		& $2\times14$			& $2 \times 14$			& $2\times32$	\\
Memory / Node		& 64GB			& 128GB				&					& 256GB		\\
Compute Cores		& 3,568		    	&14,700				& -					& 10,752 		\\
Interconnect		&				&					&					&			\\
\bottomrule
\end{tabular}
\label{tbl:hardware}
\end{table*}

\subsection{BlueCrystal phase 3 (BCP3)}
\gls{bcp3} is primarily intended for smaller jobs that run on a single node, and it is the oldest cluster still in use at the University of Bristol. A single node of \gls{bcp3} contains two, eight-core Sandy Bridge Xeon E5-2670 v1 processors, which were the first line of the Intel processors to use \gls{avx}, which increased the width of vector registers to 256-bits. 

%and a node of \gls{bcp4} contains two, fourteen-core Broadwell Xeon E5-2680 v4 processors. Although the Broadwell processor has a lower base clock rate than the Sandy Bridge processor, it incorporates more processing cores, and also has a larger memory-bandwidth. Additionally, it has wider registers due to the inclusion of the AVX-2 instruction set.

\subsection{BlueCrystal phase 4 (BCP4)}
\gls{bcp4} has been the University of Bristols main workhorse cluster since 2017. It was designed and configured by OCF in collaboration with Lenovo and is primarily intended for large parallel jobs across multiple nodes. \gls{bcp4} now has an established user-base, however the machine is almost at maximum capacity and some longer jobs can spend over a week in the queue before they run.
\par
A compute node of BCP4 contains two fourteen-core Broadwell Xeon E5-2680 v4 processors. They use the AVX2 instruction set architecture and were introduced by Intel in 2016, 

\subsection{BluePebble (BP)}
\gls{bp} is a new Intel-based cluster, managed by the \gls{acrc} at the University of Bristol. It was created in order to ease congestion on \gls{bcp4} by moving some of its heaviest users to their own cluster with dedicated resources. Some members of the meteorological research group at the University of Bristol can be classified as heavy users of \gls{bcp4}, and have recently purchased a £10,000 dedicated node of BluePebble to conduct their research using Isca. 
\par
\gls{bp} contains two different types of compute node, both using Intel's Skylake architecture. The first contains two twelve-core Xeon Gold 6126 processors and the second contains two, fourteen-core Xeon Gold 5120 processors. Both processors make use of AVX-512 instruction set. 


\subsection{Isambard}
The GW4 Alliance, which consists of the Universities of Bath, Bristol, Cardiff and Exeter, together with the UK Met Office and Cray Inc have worked together to deliver the Isambard supercomputer, which is the result of a £3m award by the \gls{epsrc}. Isambard provides multiple advanced architectures, however the focus of this research project is the Arm-based Cavium ThunderX2 processor, which forms the basis of the machine. Each of Isambard's 168 compute nodes contain 64 ARMv8 cores in a dual-socket configuration \cite{thunderprocessor2018brief}. 

%An important feature of the ThunderX2 is that it has eight memory controllers per socket, which results in a peak theoretical memory-bandwidth in excess of 250 GB/s \cite{mcintosh2018performance}. 


\subsubsection{Cavium ThunderX2 Server Microprocessors}
Arm primarily manufactures processors for mobile devices, and has only recently produced hardware optimised for HPC systems \cite{mcintosh2018performance}. Due to the heat generated by high clock rates, modern chip designers are now limited by power consumption. Because of this constraint, the current trend in supercomputer design is to use large shared-memory nodes, that use higher core cores and decreased clock rates \cite{kindratenko2011trends}. 
\par
As Arm processors were originally designed for mobile devices, they have inherently low power consumption. Because of this, the European Mont-Blanc project begun to investigate the potential of the Arm architecture for \gls{hpc} in 2011 \cite{Rajovic:2016:MPA:3014904.3014955}. This project proved to be successful, however the study uncovered some problems with the architecture that have since been addressed. ThunderX is a line of 64-bit many-core server microprocessors developed by Cavium as a result of over 8 years of work by the Mont-Blanc project and other contributors. The ThunderX2 was first released in early 2018 as the successor to the ThunderX, and is the first generation of Arm-based server microprocessors intended for high performance computing.
\par
Initial studies have found that the ThunderX2 presents as a real alternative to current offerings by vendors of desktop hardware \cite{calore2018advanced, mcintosh2018performance}, finding that the processor delivers competitive levels of performance to Intel's line of Xeon processors. 
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1\textwidth]{img/thunderx2.pdf}
\caption[Block diagram of the Cavium ThunderX2 server microprocessor]{Block diagram of the Cavium ThunderX2 server microprocessor. Diagram redrawn from \cite{thunderprocessor2018brief}}
\label{fig:thunderx2}
\end{center}
\end{figure}
\par
The ThunderX2 uses the ARMv8.1 instruction set, which allows for the use of 128-bit NEON SIMD vector registers. Perhaps the most interesting feature of the ThunderX2 as noted by McIntosh-Smith et al., is its eight memory controllers per socket, which have been demonstrated to produce a memory bandwidth in excess of 250GB/s \cite{mcintosh2018performance}. The layout of the processor is shown in Figure \ref{fig:thunderx2}.

\subsubsection{A64FX}
To meet the compute requirements of future HPC workloads, Fujitsu has recently announced the next generation of Arm chips in their A64FX processor. The A64FX improves upon the NEON instruction set found in the ThunderX2 by introducing \gls{sve}, which allow for a flexible vector register length between 128 and 512 bits so that vector length can reflect the compute requirements of different use cases \cite{stephens2017arm, rico2017arm}. These processors have not yet been released, however this thesis provides an estimate of their performance based on the performance of the ThunderX2.


\chapter{A background on benchmarking}
This chapter is an introduction to benchmarking both hardware and software, and describes some of the techniques and metrics used to benchmark the Isca code. 
\section{Cluster benchmarks}
The STREAM TRIAD and \gls{hpl} benchmarks have been used to measure the peak memory bandwidth and floating point performance of each node configuration used in this study, respectively. This has been done to provide a relative performance overview of each processor architecture, and to highlight the differences between them. 

\subsection{STREAM TRIAD}
The speed of processors has increased exponentially over the past twenty years, as described by Moore's law, which states that the number of transistors in a dense integrated circuit doubles approximately every two years  \cite{moore1965cramming}. However, the speed of memory has only marginally improved, as manufacturers have historically prioritised memory capacity over speed \cite{mccalpin1995memory,patterson1997case}. The result of this is that many scientific codes are no longer bound by compute, but by the rate at which data can be read from, or stored to memory by the processor. The STREAM memory-bandwidth benchmark was introduced by John McCalpin in 1995 to address the limitations of the benchmarks of the time, and to measure processor performance by its peak memory bandwidth consumption, rather than \gls{flops}.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/stream-triad.pdf}
\caption[STREAM TRIAD benchmark]{STREAM TRIAD results for all processor architectures that have been used as part of this research project. }
\label{fig:bandwidth}
\end{center}
\end{figure}
\par 
As core counts and memory-channels continue to grow, it becomes increasingly difficult to measure the memory bandwidth of modern processors, and results can greatly vary depending on the system configuration used to compile and run the benchmark. The results shown in Figure \ref{fig:bandwidth} were collected using the original STREAM benchmark code \cite{stream2019github}. The code was compiled using the Intel compiler with the same flags and environment variables on each cluster with the exception of the ThunderX2 processor, which used the GNU compiler. 
\par
The ThunderX2 processor has eight memory controllers per socket, and presents a peak STREAM TRIAD result in excess of 240 GB/s for a dual-socket configuration. In comparison, the Skylake processor provides a result of just 157 GB/s. This observation alone is a testament to the class leading memory bandwidth of the ThunderX2.

\subsection{High Performance Linpack (HPLinpack)}
The theoretical peak performance of a compute node can be calculated. Equation \ref{equ:perf} is commonly used to find the peak performance of a processor where $c$ denotes the processor speed in GHz, $p$ denotes the number of processor cores, $i$ denotes the number of instructions per clock cycle, and $o$ denotes the number of processors per node. 

\begin{equation}
GFLOPS = c \cdot p \cdot i \cdot o 
\label{equ:perf}
\end{equation}
\par
Generally, the theoretical peak performance of a machine is unattainable. The \gls{hpl} benchmark aims to measure the attainable percentage of peak processor performance by solving a dense system of $n\times n$ linear equations \cite{dongarra2008linpack}. Figure \ref{fig:hpl} shows the actual performance measured using the \gls{hpl} benchmark compared the the peak theoretical machine performance calculated using Equation \ref{equ:perf}. 
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.85\textwidth]{img/hpl.pdf}
\caption[High performance Linpack benchmark]{Comparison of the theoretical peak machine performance against the performance measured by HPLinpack }
\label{fig:hpl}
\end{center}
\end{figure}
\par
As can be expected, the performance measured using the HPLinpack benchmark is less than the theoretical performance in all cases. The large theoretical peak performance of the Skylake processor is attributed to its wide 512-bit vector registers, which can process 16 double precision floating point numbers with a single instruction. The drastically reduced actual performance measurement is because the processor can only use AVX-512 at a reduced clock rate, and therefore the performance model rarely opts to use these registers. 

\section{Application benchmarking}
Floating point performance and memory bandwidth are usually measured using idealised techniques like the STREAM and HPLinpack benchmarks, and may not provide the best metrics for a complex code like Isca. Although the model is computationally demanding, there is also a large overhead cost incurred by communication. This includes tasks such as reading and writing files, the movement of data into contiguous memory and the transmission of data between processors. For this reason, the model can only be expected to achieve a fraction of the theoretical peak memory bandwidth and floating point performance. Therefore, the following performance metrics have been defined, and are used throughout this study.
\subsection{Performance metrics}

\subsubsection{Wallclock runtime} 
Wallclock runtime refers to the total amount of real time that has passed from the start of the program to the end. In this study, wallclock runtime has been reported in seconds. Used alone, this metric does not provide a basis for comparison between other configurations. 
	
\subsubsection{Speedup} 
Speedup is a measure of relative performance between two solutions for the same problem. For the metric reported in this study, this is the number of times faster the code ran than some other given benchmark, typically the serial runtime. The speedup $S$ of a code can be calculated given two runtimes $R_1$ and $R_2$ using the formula in Equation \ref{equ:speedup}, whereby $R_1$ is $S\times$ faster than $R_2$.
\begin{equation}
S = \frac{R_1}{R_2} 
\label{equ:speedup}
\end{equation}
	
\subsubsection{Cost per gridpoint}
Other studies that have benchmarked parallel climate codes have used the cost per gridpoint as a primary performance metric as it takes into account the cost of interprocess communication \cite{schmidt2007benchmark}. When run on a single core, 100\% of the program runtime is spent on computation. As the number of processor cores increases, a larger portion of the runtime is spent on communication, and therefore the cpu time taken to compute a single grid-point increases. The amount of consumed compute resources $T_p$ for a given simulation can be calculated given the wallclock runtime $t$ and the number of processors used $p$, as shown in Equation \ref{equ:comp-cost}.
\par
\begin{equation}
T_p = t \cdot p
\label{equ:comp-cost}
\end{equation}
To provide a meaningful comparison between core counts, the cost per gridpoint must be calculated. Given the number of timesteps $N_t$ and number of gridpoints $N_g$, we can calculate the total simulation cost per gridpoint per timestep $C_{tg}$ as shown in Equation \ref{equ:cost}. 

\begin{equation}
C_{tg} = \frac{T_p}{N_t \cdot N_g}
\label{equ:cost}
\end{equation}
Although increasing MIMD parallelism by introducing additional processor cores decreases the overall runtime, a greater portion of the runtime is spend idle waiting for data to be sent between processors. The $C_{tg}$ metric doesn't discriminate based on wallclock runtime, and provides a solid basis for comparison between model resolution and number of processor cores. 

\subsubsection{Operational intensity}
The operational intensity $I$ of a code or compute kernel is defined as the ratio of work $W$ to the memory traffic $Q$. It is a commonly used metric to deter formally defined in Equation \ref{equ:oi}.

\begin{equation}
\label{equ:oi}
I = \frac{W}{Q}
\end{equation}
\par
For the analysis performed in this research project, $W$ denotes the number of \gls{flops}, and $Q$ denotes the total amount of memory transferred in Bytes. This results in operational intensity measured in FLOPS/Byte. 

\subsubsection{Summary}
To ensure simplicity, the wallclock runtime is the primary performance metric used throughout this paper. However, it is important to consider other metrics as they can uncover important features of the code that are overlooked by runtime alone. Values for all three metrics defined in this section have been calculated using the data collected as part of this research project. 




% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% Chapter - Porting - - - - - - - - - - - - - - - - - - - -
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\let\textcircled=\pgftextcircled
\chapter{Porting}
\label{chap:baselineexp}
In the context of software development, porting refers the process of modifying an existing codebase in order for it to run on a different system than it was originally written for. This chapter gives an overview of some of the different tools used when porting Isca, and presents some of the challenges encountered in doing so. As there are too many code small changes to present in this thesis, this chapter...

\section{Compilers and MPI libraries}
To allow for the best comparison between processors, Isca was compiled using a number of different compilers and MPI libraries. Table \ref{tbl:compilers} shows the different configurations used to compile Isca on each of the clusters used in this study. 

\begin{table}[htp]
\caption{Compilers and MPI libraries used for benchmarking}
\begin{center}
\begin{tabular}{c c c c c}
 \toprule

\textbf{Cluster} 			&	\textbf{Processor Family}		&	\textbf{Fortran Compiler}		&	\textbf{MPI library}	 \\
\midrule
\multirow{2}{*}{BCP3} 	&	\multirow{2}{*}{Sandy Bridge}	&	GNU 7.1.0 			&	OpenMPI	 \\
					& 							& 	Intel 13.0.1			&	OpenMPI			\\
\midrule	
\multirow{2}{*}{BCP4}	&	\multirow{2}{*}{Broadwell}		&	GNU 7.2.0			&	OpenMPI			\\
	&											&	Intel 18.0.3			&	Intel MPI			\\
\midrule	
BP					&	Skylake					&	Intel 19.0.3 			&	Intel MPI			\\
\midrule			
\multirow{2}{*}{Isambard}	&	\multirow{2}{*}{ThunderX2}	&	CCE 8.7.9				&	Cray MPI			\\
					&							&	GNU 8.2.0			&	Cray MPI			\\
%					&							&	Arm $x.y$				&	Cray MPI			\\
\bottomrule \\
\end{tabular}
\end{center}
\label{tbl:compilers}
\end{table}%

\subsection{GNU Compiler Collection (GCC)}
The \gls{gcc} is a selection of compilers for numerous programming languages produced and maintained by the GNU project. 

\subsection{Intel Compiler Collection (ICC)}
The \gls{icc}


\subsection{Cray Compiling Environment (CCE)}
The \gls{cce} is a Fortran 90 compiler developed by Cray Inc. This compiler is relatively new in comparison to the Intel and GNU compilers, and as such is strict to the Fortran standard. This caused many issues when porting the code using this compiler, and flagged up a number of issues with the Isca codebase. However, the process of Porting for CCE turned into a stringent debugging exercise that has improved the reproduciblity of the code on different platforms. The following section describes some of the code changes required to compile and run Isca using the CCE compiler. A full list of code changes can be found in appendix TODO.

\subsubsection{Implicit type conversion}
To provide interprocess communication, Isca uses a `\textit{Massively-parallel} ' module, codenamed MPP. It is a set of simple calls to provide a uniform interface to a collection of commonly used message-passing routines for climate modelling, implemented in different libraries \cite{balaji2002fms}. This module defines many subroutines that depend on the definition of the \texttt{MPP\_DEFAULT\_VALUE\_} macro, which is defined using preprocessor directives at compile time. The \texttt{MPP\_DEFAULT\_VALUE\_} can be assigned as either real, integer or logical. As an extension for backwards compatibility with other compilers, the GNU and Intel compilers allow for the implicit conversion of logicals to integers and reals and vice versa. When converting from a logical to an integer, the numeric value of \texttt{.false.} is 0, and that of \texttt{.true.} is 1. When converting from integer to logical, the value 0 is interpreted as \texttt{.false.} and any non-zero value is interpreted as \texttt{.true.}. This does not conform to the Fortran 90 standard, which disallows implicit conversion between numeric variables and logicals \cite{fortran1991standard,gnu2019conversion}.




\section{Libraries etc}
The NetCDF library installed on BCP4 was incompatible with the latest GNU Fortran compiler, therefore the NetCDF library was installed in the \texttt{\$HOME} directory in order to compile Isca using this configuration. On bcp4 the FFTW library was installed using easybuild, and wastherefore slow, so i reinstalled in my home directory. 

\section{BlueCrystal phase 3}
The NetCDF library installed on \gls{bcp3} was incompatible with the latest GNU Fortran compiler, therefore the NetCDF library was installed in the \texttt{\$HOME} directory in order to compile Isca using this configuration. 

\section{Discussion}
A large part of the Isca code does not adhere to the Fortran standard. 

Additionally, the Fortran standard has evolved over the past twenty years, and many features that were once commonplace are no longer supported by newer compilers. Some of the more popular compilers like the GNU Compiler Collection (GCC) and the Intel Compiler Collection (ICC) have been updated to allow for backwards compatibility with legacy code, however this is not the case for the Cray and Arm compilers that are available on Isambard and Catalyst.
\par
The FMS was first released in 1998. It is a slowly evolving code that relies on 
\par
Isca is a strong scaling code. This means that the total program runtime varies with the number of processor cores for a fixed total problem size. The design of the code means that it is very limited in how it can scale. 


\par









\section{BluePebble}

\subsection{Building modules}

Had to build: 
\begin{itemize}
	\item Python 3.7 on Catalyst
	\item NetCDF on BluePebble
	\item Git on BluePebble
\end{itemize}

\subsection{Build Tools}
BluePebble only has access to the Intel Fortran and C compilers, and only allows for the use of the Intel MPI implementation. This was a purposeful decision made when designing the cluster with the aim of reducing the number of libraries, and therefore the number of dependencies for each code base. This meant that the Isca codebase had be compatible with the latest 2019 version of the Intel compiler. 
\par
As BluePebble is still in its Beta phase, there are only a limited number of modules on the system. In order to compile Isca, the NetCDF, Git, and Anaconda Python libraries had to be built and made into module files. 

\section{Issues}
The default stack size on Bluepebble was 8Kb. However, as Isca uses a large amount of memory, this caused a stack overflow error when running the model at resolutions greater than T42. Due to some configuration restraints on the cluster, the PBS scheduler does not allow for the stack size to be increased using \texttt{ulimit -s unlimited}, as used in Isca's run script. To resolve this issue, the default stack size was increased to 64k.
\par
As a temporary work-around before this issue was resolved, and before interactive jobs were available on the cluster, a regular job can be submitted the queue that sleeps for an hour in the submission script. The details of the job can be found using the  \texttt{qstat -f <jobid>} PBS command, which can then be used to SSH to the node running the sleeping job. As PBS has been not been used to get access to the node, the stack size can be increased using the command \texttt{ulimit -s unlimited}, and the code will then run as if in an interactive job. Most of the runtimes for BluePebble were collected using this technique. 


% Isambard porting
\section{Isambard}
Although many of Isambards compute nodes use the Cavium ThunderX2 processor, the machine itself is a Cray XC50. This means that there are a wide range of tools and compilation environments available on this cluster that are unique to Cray systems. 


\subsection{GNU Compiler Collection}
The GNU compiler collection is commonly used... 






\par
This implicit conversion allowed by the GNU and Intel compilers caused many errors for the CCE compiler. To resolve this issue, additional preprocessor directives had to be included in the \texttt{mpp\_domains\_reduce.inc} file. 

\subsubsection{Namelist read errors}
Isca uses Fortran namelist files to read large numbers of parameters into existing variables and data structures at runtime. 
\par
The code exhibited different behaviour when compiled using CCE, which suggests that the code relies on some legacy compiler features that are not part of the original Fortran standard, and have remain in the Intel and GNU compilers for backwards compatibility issues. 

\subsection{Ambiguous arithmetic}
The Cray compiler required brackets around some arithmetic. 





\subsubsection{Arm HPC Compiler}
The Arm HPC compiler....


\subsection{Modifications to codebase}
On Cray machines, jobs submitted using the PBS pro workload manager must launch MPI applications using the \texttt{aprun} utility, which is synonymous to the \texttt{mpirun} utility provided by Intel MPI. Prior to this research project, Isca had never been compiled and run on a Cray machine, and the script template used to execute the compiled binary (\texttt{run.sh}) only allowed for the use of \texttt{mpirun}.
\par
To allow for the code to run using \texttt{aprun}, a clause was added to the \texttt{run.sh} template script that looks for an environment file called \texttt{EXECUTION\_TYPE}, which must be set the value \texttt{APRUN} if running on a cray system. To allow for backwards compatibility, the script defaults to using \texttt{mpirun} if no environment file is defined. 

\begin{lstlisting}[language=bash]
  if [[ -z "${EXECUTION_TYPE}" ]] || [ "${EXECUTION_TYPE^^}" = "MPIRUN" ]; then
    exec nice -{{nice_score}} mpirun {{mpirun_opts}} -n {{ num_cores }} {{ execdir }}/{{ executable }}
    
  elif [ "${EXECUTION_TYPE^^}" = "APRUN" ]; then
    eexec nice -{{nice_score}} aprun {{mpirun_opts}} -n {{ num_cores }} {{ execdir }}/{{ executable }}
  fi
\end{lstlisting}
A full list of code changes can be found in appendix \ref{na}. 





\section{Verification of results}
When porting a codebase, it is important to test that that the changes made to the code are backwards compatible. This means that changes must be non-intrusive, and configurations must default to the original behaviour. In the case of Isca, the code changes that have been made to run on the new machine must not stop the code from working on an old one. 
\subsection{Units of last place}
To ensure that the model produces the same results on each system, the model outputs have been checked using the \gls{ulp} numerical analysis technique, which can be used to measure the spacing between floating point numbers. As the whole real line cannot be represented in computer memory, there is a minimum distance between two numbers occupying the full space offered by floating point numbers. NetCDF files aim to record data in a continuous fashion, which is an impossible task for the discrete numbers used in computational simulations like Isca.
\par
The outputs of a simulation run by Isca can be verified by comparing the ULP of each parameter in the resulting NetCDF file. As the model is chaotic, even small changes to model parameters can produce vastly different results, although as the model is not stochastic in any way, the same simulation configuration ran on two separate machines should produce identical results, assuming that the compilers use the same amount of memory of each floating point variable. 

\subsection{Rounding errors}
Rounding errors are a commonly occurring quantisation problem in scientific codes (citation needed). Although double-precision floating point variables can store numbers to a high degree of precision, they can only store a finite number of digits. Rounding errors are a result of performing arithmetic on continuous numbers in a discrete representation. The IEEE Standard for Floating-Point Arithmetic (IEEE 754) states that all floating point arithmetic must be correctly rounded to within 0.5 ULP of the true mathematical result, therefore any differences greater that 1 ULP suggests an inconsistent result. 

\subsection{Application}
A C++ program written by Gethin Williams in 2009, and modified for this research project was used to measure the difference in ULP between NetCDF files obtained on different machines. The program allows for a tolerance to be given, to accomodate any rounding errors that may have accumulated throughout the course of the simulation. Due to the differences in compiler and processor architecture, a tolerance of 2 ULP was deemed acceptable. All changes to the Isca codebase were verified using this metric.
\par






\part{Benchmarking, performance analysis and optimisation}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% Chapter - Benchmarking and Scaling study - - - 
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\chapter{Benchmarking and Performance Analysis}
This chapter presents an extensive performance analysis of the Isca model running two unique configurations. The aim of this is to characterise the code on different hardware configurations, aid in the identification of performance optimisations and to provide a comparison of the processors themselves. To do this, the following experiments were performed:

\begin{description}

	\item[Scaling study] The runtimes of two model configurations have been measured on four different compute nodes. This involved varying the core count and spatial resolution on each system.
	
	\item[Compiler comparison] The per-node performance of the model was compared for two different compilers on each processor, excluding Skylake.
	
	\item[Performance projection] Using the results from the scaling study, the performance of Isca has been projected to Fujitsu's A64FX Arm processor, which is planned to be debuted in the Post-K supercomputer in 2021. Most notably, the A64FX will be the first production processor to use the new \gls{sve}, which allow for SIMD registers of up to 512-bits.
	
\end{description}

\section{Model configurations}
Isca is a coupled model, allowing for the simulation of either the atmospheric or oceanic components of a planet, or both components simultaneously. The complexity of these simulations are defined at compile time, and allow for different algorithms to be applied depending on the model configuration. Although this means that the model is highly flexible, it introduces a challenge when trying to profile the code as a whole, as optimising one configuration may have no impact on another. This research project focuses on the optimisation of two test configurations: the well-known Held-Suarez configuration and a Grey-Mars radiation model.

\subsection{Held-Suarez test case}
The Held-Suarez simulation was designed by Held and Suarez in 1994 \cite{held1994proposal} to allow for comparison between \gls{gcm}s. It is well studied, and is considered to be the gold standard for benchmarking climate modelling code (citation needed). It is configured to simulate only the `dynamical core' of a planet, which contains the discretised equations of motion and state. In terms of complexity, the Held-Suarez model is one of the simplest configurations available, and is essentially the foundations upon which more complex models are built. The simulation maintains a constant climate throughout its duration by forcing many parameters to predefined values. This allows for the dynamical core to be run by itself, without the need for coupling with other complex model components. This makes the Held-Suarez a good candidate for benchmarking and optimisation as the dynamical core code is used in all other model configurations that model the atmosphere. 
\par
Isca's Held-Suarez simulation computes over an idealised model of the Earth. In terms of measuring its performance, the simulation has been run for 12 model months with each month simplified to last 30 days, for a total of 360 model days per simulation. This length of time was chosen to allow for the performance to be measured at each phase of the Earth's orbit of the Sun. The Held-Suarez simulation does not use solar radiation as a model parameter. However, it is important to measure the performance for a full year to model other seasonal parameters.

\subsection{Grey-Mars test case}
The Grey-Mars simulation is configured to simulate the effect of grey radiation on the planet Mars over time, building upon the dynamical core code used in the Held-Suarez configuration. It was chosen for optimisation due to its frequent use by academics at the University of Bristol, as well as its demonstration of some of the more complex features of the model. 
\par
The axes of both Earth and Mars are not orthogonal to their orbit of the sun. Earth's axis is at a $23.5^{\circ}$ tilt, and Mars' axis is at $25^{\circ}$ (ref).  These tilted axes are responsible for the seasons, however this causes many climate models to suffer from a load-balancing issue whereby calculations take longer on the side of the planet facing the sun due to increased levels of thermal radiation (ref). To test for this, the Grey-Mars configuration has been run for 690 model days to account for the 687 martian days it takes for Mars to orbit the Sun. This simulation is broken into 23 sub-simulations, each lasting 30 days. 

\subsection{Domain decomposition}
When running in parallel, Isca requires that the number of latitudes divided by the number of cores must be divisible by 2 \cite{isca2019github}. Therefore the T21 resolution, which splits the planetary domain into 32 latitudes, can be run on 1, 2, 4, 8 or 16 cores. The simulation cannot be run on more processor cores, as the number of processors will be equal to the number of latitude bands. A full list of compatible resolutions and core counts can be found in Table \ref{tbl:resolutions}.

\begin{table}[htbp]
\caption[Resolutions and their compatible core counts]{Resolutions and their compatible core counts. Lower resolutions are limited in the number of cores they can use.}
\begin{center}
\begin{tabular}{ c c c }
\toprule
Truncation 	& Latitudes$\times$longitudes 		& Available core count \\\midrule
 T21 			& $32\times64$					& 1, 2, 4, 8, 16 \\  
 T42 			& $64\times128$				& 1, 2, 4, 8, 16, 32 \\
 T85 			& $128\times256$ 				& 1, 2, 4, 8, 16, 32, 64 \\
 T170 		& $256\times512$ 				& 1, 2, 4, 8, 16, 32, 64, 128    \\\bottomrule
\end{tabular}
\label{tbl:resolutions}
\end{center}
\end{table}
\par
This inherent domain decomposition constraint imposed by the model means that some nodes are unable to run Isca at full capacity. For example, a single node configuration of BCP4 can only run  Isca on 16 out of 28 cores per node. This poses an interesting problem, whereby the model is a better fit for some nodes than others, simply due of the number of processor cores per node. 
\par
Although Isca can vary both in its spatial and temporal resolution, the scaling study undertaken as part of this research project focuses solely on variations in spatial resolution. This decision was made in order to simplify the process of performance modelling by limiting the number of problem sizes. Additionally, changes to performance as a result of time stepping are generally predictable, and will not contribute to a further understanding of the code. A model that performs twice as many time steps will perform twice as many calculations and will therefore be twice as slow. 

\section{Automated data collection}
In order to collect reliable and consistent data, it is important to define a strict method of data collection. When benchmarking high performance applications, data collection can be a timely process. The runtimes of the configurations used in this study are in the range of 3 minutes for simple configurations at low resolutions, up to 10 days for high resolution complex scenarios running in serial. Running this range of simulations manually would be incredibly time consuming, therefore a Python library was written to automate this process. The source code for this library can be found on GitHub (citation). 
\par
The Python library was written to sequentially run a number of different experimental configurations given a set of parameters, including the core count, resolution and model configuration. This allowed for a number of experiments to be run from within a single job submission script, with the results of each experiment automatically stored in a spreadsheet. Each experiment defined by the Python script recorded the total time taken to complete the simulation, as well as each thirty-day time step. 
\par
In order to collect reliable data, a full node was used for each experiment. This means that even when run in serial, the model used the resources of an entire node, so that the performance would not be affected by shared resource usage by other programs running on the cluster. To account for variation in runtime caused by factors outside the control of the experiment, all runtimes reported in this chapter are the mean value of three repeat measurements. The results presented in the following section are the consequence of over 2,000 hours of experimental runtime.


\subsection{Job submission}
\gls{bcp3} uses the \gls{pbs} job scheduler, \gls{bp} and Isambard use the \gls{pbs} Pro job scheduler and \gls{bcp4} uses the SLURM scheduler. These are tools that allow for applications to be submitted to a queue, and then run on a compute node when the required resources are available. Each of these schedulers use a slightly different syntax, therefore a number of submission scripts have been created for each cluster based on the amount of resources required and expected runtime. Example job submission scripts can be found in Appendix \ref{apx:submission}. 
\par
As the clusters used in this project are actively used for research, there is naturally some competition for compute resources between users. A trial and error approach was used to find the right parameters for the job script in order for the job to be processed from the queue quickly, whilst ensuring that the runtime was adequate to complete the entirety of the job.


\section{Scaling study}
To determine how well the model performs when presented with additional compute resources, Isca was run on 1 core, up to and including the maximum number of cores available on a node of each cluster. In addition to this, it was also run across all possible combinations of model configuration and resolution to model its performance at various levels of complexity and realism.  
\par
Whilst the majority of the results presented in this section were to be expected, the performance is significantly affected when running across multiple nodes. For this reason, the results have been presented as a comparison of the performance on a single node, and a further analysis has been done for the multi node configurations.

\subsection{Results and discussion}
For the Held-Suarez configuration, each spatial resolution was run up to the maxmum. 
\par
The Grey-Mars simulation can only be run on a maximum resolution of T42. 

\begin{description}
	\item[Held-Suarez T21: ] 1, 2, 4, 8, 16
	\item[Held-Suarez T42: ] 1, 2, 4, 8, 16, 32
	\item[Held-Suarez T85: ] 1, 2, 4, 8, 16, 32, 64
\end{description}

\subsubsection{T21 resolution}
Why are lower resolutions good?
\par
The scaling curve as shown in Figure \ref{fig:t21-scale} shows how the runtime varies as a function of processor cores for the Held-Suarez configuration at the T21 resolution, and can be described as a sublinear plateau for all node configurations. The wallclock runtime is greatest when the program is run in serial, and lowest when run on 16 cores, which is the maximum number of cores possible for this resolution.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{img/scaling_graph_T21_Held_suarez.pdf}
\caption{Wallclock runtime of the Held-Suarez configuration running at T21 resolution across all processor architectures. The vertical purple bar on the $y$ plane indicates the maximum number of processors available on a node of BCP3.}
\label{fig:t21-scale}
\end{center}
\end{figure}
\par
Interestingly, when increasing the number of processor cores from 8 to 16 in the Held-Suarez case, there is only an approximate 1.15$\times$ performance gain for the Intel processors. Results like these are typical of many parallel codes, whereby the performance benefit of additional compute resources decreases as more processor cores are utilised. This effect is similar to the law of diminishing returns observed in Economics. As this problem size is small, more time is spent in communication relative to compute when a greater number of processor cores are used.
\par

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8\linewidth]{img/scaling_graph_T21_Grey_mars.pdf}
\caption{Wallclock runtime of the Grey-Mars configuration across all processor architectures. Vertical coloured bars on the $y$ plane indicate the maximum number of processors available on a node for each cluster.}
\label{fig:t21-scale-grey}
\end{center}
\end{figure}
\par
The trend observed for the Grey-Mars configuration in Figure \ref{fig:t21-scale-grey} is comparable to that of of the Held-Suarez result, whereby the slowest runtime is the serial case, and the fastest is the 16 core case. The Skylake processor massively outperforms all other processors when run on 8 cores, however it quickly tapers off when run on 16 cores, providing just a 1.02$\times$ speedup. 
\par
To determine the importance of memory bandwidth at this resolution, the 16 core configuration was rerun on the Broadwell processor. However, the cores were split across two nodes. Figure x shows that...x


\subsubsection{T42 resolution}
The T42 resolution in Figure \ref{fig:t42-scale} presents a similar scaling curve to that observed for the T21 resolution. For all processors except Sandy Bridge, the slowest runtime is measured for the serial code, and the performance improves until the program is run on 16 cores. However for the Intel processors, running on more than 16 cores requires multiple nodes, and this has a dramatic impact on the performance of the Sandy Bridge and Skylake processors. 
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{img/scaling_graph_T42_Held_suarez.pdf}
\caption{Wallclock runtime of the Held-Suarez configuration running at T42 resolution across all processor architectures. The vertical coloured bars on the $y$ plane indicate the maximum number of processor cores available on a node of each cluster.}
\label{fig:t42-scale}
\end{center}
\end{figure}
\par
In comparison to the ThunderX2, which still runs within a in a single socket when using 32 processor cores, we can determine that this issue is most likely caused by internode communication rather than a problem with the model itself. Interestingly, this behaviour is not apparent on the Broadwell processor, which improves its performance as expected when increasing the number of processor cores used from 16 to 32, even though two nodes are used in this case. To determine the cause, will need to to a mpi trace.
\par

\par
The worst performance observed for the multinode configuration is the Sandy Bridge processor, which takes 82$\times$ longer to complete the same job than when running on a single node. Both BCP3 and BluePebble are intended to be used for smaller node jobs on a single node, so this result could be explained by a slower interconnect. 
\par
Generally, running on a greater number of nodes improves the performance of codes that are bound by memory bandwidth, as a greater number of nodes implies a larger memory bandwidth. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\linewidth]{img/speedup-T42-Held_suarez.pdf}
\caption{Wallclock runtime of the Grey-Mars configuration across all processor architectures. Vertical coloured bars on the $y$ plane indicate the maximum number of processors available on a node for each cluster.}
\label{default}
\end{center}
\end{figure}

\newpage
\subsubsection{T85 resolution}
Due to restrictions imposed by domain decomposition, the Grey-Mars configuration cannot be run at a resolution higher than T42. Because of this, the results for the T85 resolution are limited to the Held-Suarez configuration only. Additionally, the time limit imposed by the queuing system prevented results from being gathered for the Sandy Bridge processor when running on 64 cores as the runtime was longer than 360 hours. 
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{img/scaling_graph_T85_Held_suarez.pdf}
\caption{Wallclock runtime of the Held-Suarez configuration running at T85 resolution across all processor architectures. The vertical coloured bars on the $y$ plane indicate the maximum number of processors available on a node of each cluster.}
\label{fig:t85-scale}
\end{center}
\end{figure}
\par	
Plotting the scaling graph as a function of speedup in relation to the serial performance, we can see that...
When run on 64 cores across four nodes, the performance of the Broadwell processor 

The results obtained for the T85 resolution are similar to the T42 resolution
A similar behaviour is observed for the largest problem size of T85. 
\par
One possible reason for this phenomenon is that the MPI library is using shared memory for communication. 
\par
The BluePebble and BCP3 supercomputers were designed to be used for single node jobs, and larger communication times may be as a result of the interconnect? 




\subsection{Conclusions}
This scaling study highlighted the differences in processor architecture between Arm and Intel. Arms approach to processor design relies on a greater number of simpler cores, in comparison to Intel's fewer more complex cores. This design benefitted the performance of Isca when run on the ThunderX2 by keeping communication inside a single node for all resolutions tested and allowed for competitive runtimes to that of the Intel processors.
\par
An interesting issue has been uncovered on the BCP3 (Sand Bridge) and BP (Skylake) supercomputers, whereby Isca performs worse when run in a multi-node configuration. Further analysis of this problem has been discussed in section x. Although the different run configurations have demonstrated 
\par
BluePebble is slow because it just uses ethernet. 

Although the ThunderX2 has the largest memory bandwidth, it does not have the best runtime at all resolutions. 

It would be interesting to run the model at much higher resolutions to see how it reacts to processors at very high core counts.





\section{Compiler comparison}
To find the optimal compilation settings for each processor, both the Held-Suarez and Grey-Mars model configurations were complied using two different compilers on each cluster. All cases compiled using the GNU compiler used the same flags, and all cases compiled using the Intel compiler used the same flags. At the time of writing, only the Intel compiler and MPI library was available on BluePebble, therefore there is no comparison of different compilers on a Skylake node. 
\par

\begin{table}[htp]
\caption{Number of processor cores used to measure the performance of different compilers at the T21 and T42 resolutions.}
\begin{center}
\begin{tabular}{c c c}
\toprule
\multirow{2}{*}{\textbf{Processor Family}}	&	\multicolumn{2}{c}{\textbf{Number of cores}}	\\
							 		\cmidrule(lr){2-3}
								& 	\textbf{T21} 	&	\textbf{T42}			\\
\midrule
Sandy Bridge						&	16			&	16					\\
Broadwell							&	16			&	16					\\
ThunderX2						&	16			&	32					\\
\bottomrule
\end{tabular}
\end{center}
\label{tbl:-compiler-cores}
\end{table}%

\par

For this test, the per-node performance was considered only. This means that the model  was ran up to the maximum number of cores available on each node for the given model configuration as shown in Table \ref{tbl:-compiler-cores}. This was done to provide a comparison of the compilers in relation to the performance available on other processors.  
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1\textwidth]{img/compiler-comparison.pdf}
\caption{Runtimes for the Held-Suarez and Grey-Mars configurations using different compilers.}
\label{fig:compiler-comparison-t42}
\end{center}
\end{figure}
\par
Figure \ref{fig:compiler-comparison-t42} shows that for all configurations tested on Intel nodes, the Intel compiler outperformed the GNU compiler. This is consistent with what was expected, as Intel has developed their compiler alongside their processors and it therefore produces well-optimised instructions. For the ThunderX2, the GNU compiler outperformed the Cray compiler. Neither compiler is developed specifically for the ThunderX2, therefore this performance difference can be explained by the fact that the GNU compiler is older and therefore more mature than the Cray compiler, however this is just speculation and has not been investigated. To confirm these observations, the test was repeated for the T21 resolution, which yielded similar results. 



\section{Communication analysis}
Load imbalance refers to an uneven distribution of work across compute resources. In the domain of HPC, it affects the performance of parallel codes only.
\par
The rate of computation is defined by the cost of computing a single grid cell per time step of the simulation, relative to the number of processors used. When run in serial, 100\% of the program runtime is spent on compute as there is no interprocessor communication. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1\textwidth]{img/cost-per-grid-point.pdf} 
\caption{Cost per grid point for the Held-Suarez and Grey-Mars configurations at T21 and T42 resolutions.}
\label{fig:cost-per-grid-point}
\end{center}
\end{figure}
\par
The measured rate of computation for the three model resolutions is shown in Figure \ref{fig:cost-per-grid-point}.



\section{Performance projection}
The A64FX processor
\begin{itemize}
\item list processor specs
\item compare specs to Thunderx2
\item Does the A64fx clock down?
\end{itemize}
Using these results, how do I think that the code will perform on the new processors?


%\section{Throughput}
%How to maximise throughput? 
%
%\section{Job Scripts}
%
%
%\subsection{Portable Batch System (PBS)}
%BlueCrystal phase 3 uses the PBS queuing system.
%(example of PBS job script)
%
%\subsection{PBS pro}
%Both Isambard and BluePebble use the PBS pro queuing system
%
%\subsection{SLURM}
%(example of SLURM job script)
%
%
%\section{Results}
%\subsection{Single node performance}
%
%The code doesn't scale outside of a node. It is limited by both the resolution of the model and the number of cores on a node. This actually bodes well for the Arm nodes, as the model decomposes nicely onto 64 cores. To test the total throughput we need to measure the number of model days simulated per hour or something similar to that.
%
%
%
%
%\subsection{Whole node performance }
%
%\subsection{Cache misses etc}
%\subsection{Vectorisation}
%Some of the most time consuming loops (spherical\_fourier.F90), operate on variables of a double precision complex type. These variables contain two double precision floating point numbers, both occupying 8 bytes of storage, for a total storage space of 128bits. This is wider than the vector registers...
%\par
%By default, Isca uses 8-byte real and integer
%\par
%Talk about how AVX-512 is usually slower, because it decreases the clock speed in order to prevent overheating. 
%These variables  loops use a 
%\par






% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% Chapter - Performance Analysis and Optimisation 
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\let\textcircled=\pgftextcircled
\chapter[Optimisation]{Optimisation}
\label{chap:optimiations}
This chapter outlines the various performance bottlenecks identified in the Isca codebase, describes the steps taken to address their underlying issues, and evaluates the performance of the code changes that have been made. In some cases, a significant performance improvement was observed. 

\section{Roofline model analysis}
A roofline model is an insightful visual performance analysis technique used to identify the hardware-limiting factors of an application, or compute kernels within an application. It plots the floating point performance as a function of peak machine performance, peak machine memory-bandwidth and the operational intensity of the code itself. The performance limiting factor of a code or compute kernel can be determined by looking at where it sits on the roofline.

\begin{figure}[htbp]
\begin{center}
 \includegraphics[width=\textwidth]{img/roofline_model_bluepebble.pdf}
\caption[Roofline model of Isca on Intel hardware]{Roofline model for 16 cores of two Intel Xeon Gold processors at 2.6 GHz. } 
\label{fig:roofline}
\end{center}
\end{figure}
\newpage

\subsection{Isca performance}
The Held-Suarez simulation running at the T42 resolution delivers an operational intensity of 0.11 FLOPS/Byte and a double-precision floating point performance of 1.54 GFLOPS. The roofline model in Figure \ref{fig:roofline} indicates that the configuration is limited by memory-bandwidth, as the program total performance is located underneath the DRAM ceiling. Intel Advisor suggests that the theoretical peak double-precision performance of a code running on the Skylake architecture using SIMD is 584.99 GFLOPS. This means that the Isca code is only running at 0.26\% of the peak performance available to the hardware.
\par
In addition to quantifying the performance of Isca, Figure \ref{fig:roofline} identifies 2 compute kernels referred to as Loop $a$ and Loop $b$, for optimisation. Loops $a$ and $b$ are both found in Isca's FFT code, and provide especially bad floating-point performance. The following section investigates the limitations of these kernels and describes how they have been improved.


% FFT Optimisation
\section[FFT optimisation]{Fast Fourier Transform optimisation}
As discussed in Section \ref{sec:spectral-domain}, spectral climate models use a FFT to transform data between the spacial and frequency domains. Isca does this using the \texttt{fft991} subroutine, found in the \texttt{fft99.F90} module. This subroutine is used to perform multiple one-dimensional FFT's in succession over a two-dimensional array of sequential data when converting from a grid-point decomposition to the frequency domain and vice versa. This implementation of the FFT has been adapted from a Fortran77 code written by Clive Temperton in 1981, and can be found in the EMOSLIB library by The European Centre for Medium-Range Weather Forecasts (references).
\par
\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{img/tikz-img/vector_registers_contiguous/vector_registers.pdf}
   \caption{}
   \label{fig:Ng1} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{img/tikz-img/vector_registers_noncontiguous/vector_registers.pdf}
   \caption{}
   \label{fig:Ng2}
\end{subfigure}

\caption[Contiguous and non-contiguous memory access]{ (a) demonstrates that four contiguous single-precision floating point numbers can be read from memory with a single AVX-2 instruction, whilst (b) shows how four separate loads would be required for the same operation for non-contiguous data with a stride of 4.}
\label{fig:vector-registers}
\end{figure}

Although the \texttt{fft991} subroutine includes preprocessor directives to ignore vector dependencies at the most time-consuming loops, neither the Cray, GNU or Intel compilers will perform automatic vectorisation. This results in four loops in the \texttt{fft99.F90} module being run as scalar code, even when vectorisation is possible. Intel Advisor indicates that this is caused by a fixed-width iteration through multiple data structures using a non-contiguous stride.
\par
In the case of the 256-bit wide vector registers found in BCP4's Broadwell processors, eight consecutive floats, or four consecutive doubles may be loaded from memory with a single AVX-2 instruction. However, if the memory locations are not adjacent, then they must be loaded using multiple instructions, negating the benefit of using vector registers. This is illustrated in Figure \ref{fig:vector-registers}. 
\par
When forcing the compiler to use vector instructions by using the \texttt{!DIR\$ VECTOR ALWAYS} preprocessor directive, there is only a marginal improvement over the scalar code, as shown in Table \ref{tbl:force-vec}. This provides evidence to support that the code has not been written to vectorise on modern hardware. Also, was compiled using xHost. Perhaps in the 80's the cost of the non-contiguous memory access was not as high? look into this...

\begin{table}[htp]
\caption{Runtime spent inside two compute kernels for both scalar and vector code}
\begin{center}
\begin{tabular}{ c c c }
\toprule 		
Loop						&	Scalar time (milliseconds)	&	Vector time (milliseconds)	\\ 
\midrule
($a$) \texttt{vpassm:1081}		&	822.5				&	649.8				\\
($b$) \texttt{vpassm:1049}		&	793.6				&	459.8				\\
\bottomrule

\end{tabular}
\end{center}
\label{tbl:force-vec}
\end{table}

One of the biggest problems with Temperton's FFT is that it performs the transformation in-place. Although this reduces memory consumption, it introduces additional algorithm complexity as the results of intermediate calculations are not written to a temporary array. This may have been important in the late 80's and early 90's when memory was in short supply, however modern processors often have in excess of 18 MB of on-chip cache and the total amount of memory usage is rarely an issue. 
\par
Although it may possible to rewrite Temperton's FFT to better make use of vector registers, this would be a massively time-consuming task, and does not guarantee a performance improvement. Therefore, modifications to the codebase have been made to allow for the use of the \gls{fftw} library instead.


\subsection{FFTW}
FFTW is an implementation of a \gls{dft} that aims to adapt to the hardware on which it is run \cite{frigo2005design}. The library has been written in ANSI C, however it provides interfaces for other programming languages including Fortran. Rather than providing a hand-tuned implementation for all possible hardware configurations, FFTW uses a plan to precompute various sub-arrays based on the shape, size and memory layout of the input data, without requiring the data itself \cite{frigo2005design}. The planning process yields a plan, which is an executable data structure that returns the DFT of the given input data. 
\par
To create a plan optimised for the hardware on which the code is compiled, the planner measures the runtime of many different plan configurations, returning the plan which results in the quickest runtime \cite{frigo2005design}. The planning process is computationally expensive, however it is only performed once, and the resulting plan can then be reused on different input data of the same dimensions. If many FFT's of the same type are repeatedly called in an application, this generally provides a net performance gain.
\par
Plans are created using FFTW's own compiler called \texttt{genFFT}. Whilst the FFTW library itself is written in ANSI C, \texttt{genFFT} is written in Objective Caml, and is used to produce a number of small hard-coded transforms called codelets. Codelets are well-optimised simple straight line programs, which compute the DFT of a small sequence of data. The speed of FFTW is largely accredited to these codelets, which are sucessively applied to sections of a larger sequence. 
\par
Although not a requirement of using FFTW, the input data should be contiguous in memory so that vector instructions can be exploited. FFTW Version 3.3.8 officially supports AVX x86 extensions and Version 3.3.1 introduced support for the ARM Neon extensions \cite{frigo2003fftw}. Version 3.3.8 of the library was chosen so that it targets the vector extensions on all hardware configurations used in this research project. 


\subsubsection{Cooley-Tukey algorithm}
Despite FFTW using many different FFT algorithms, the most commonly used is the Cooley-Tukey algorithm. This algorithm was popularised in 1965, however variations of the algorithm have been known as early as 1805 \cite{cooley1965algorithm, heideman1985gauss}. Proper implementation of the Cooley-Tukey algorithm results in a time complexity of $O(n\, \textrm{log}\, n)$. The algorithm is based on the assumption that a DFT of size $n = n_{1} \cdot n_{2}$ can be expressed as a two-dimensional DFT of size $n_{1} \times n_{2}$. The algorithm itself can be broken into three steps: 

\begin{enumerate}
	\item Perform $n_{1}$ DFTs of size $n_{2}$;
	\item Multiply by some \textit{twiddle factors}, which are a constant complex coefficient that is multiplied by the input data in order to recursively combine small DFTs;
	\item Perform $n_{2}$ DFTs of size $n_{1}$.
\end{enumerate}

When presented with this information, it becomes clear why the authors of FFTW decided to use a codelet-based design. An optimal solution to performing the FFT using the Cooley-Tukey algorithm allows for a codelet to calculate the DFT on a number of data structures of either size $n_{1}$ or $n_{2}$.
\par
The difference between the forward and backwards transform is the sign of the exponent. \url{http://www.fftw.org/fftw2_doc/fftw_3.html}
\begin{equation}
Y _ { i } = \sum _ { j = 0 } ^ { N - 1 } X _ { j } e ^ { - 2 \pi i j \sqrt { - 1 } / N }
\end{equation}
\begin{equation}
Y _ { i } = \sum _ { j = 0 } ^ { N - 1 } X _ { j } e ^ { 2 \pi i j \sqrt { - 1 / N } }
\end{equation}

\subsubsection{FFTW applied to Isca}
In order to call FFTW rather than Temperton's FFT, a new Fortran module \texttt{fftw.F90} has been written, and preprocessor directives have been added to the existing \texttt{fft.F90} file to allow for the type of FFT used to be chosen at compile time. Compiling the model with the \texttt{-DFFTW3} preprocessor directive will compile the model to use \gls{fftw} instead of the default call to Temperton's FFT. Isambard, \gls{bcp3}, \gls{bcp4} and \gls{bp} have module files that allow for automatic linking to the FFTW library. If using the library on other systems, the FFTW library must be installed and linked to Isca manually. FFTW provides both a single and double precision version of its library, with subtle differences to the names of the interfaces it provides. Preprocessor directives must be used to chose which version of FFTW is linked to the Isca code. 
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{img/tikz-img/data_layout/data_layout.pdf}
\caption[Two-dimensional data layout in Fortran]{Contiguous data layout in memory for a two-dimensional array in Fortran.}
\label{fig:memalign}
\end{center}
\end{figure}
\par
To guarantee proper alignment for SIMD, the data structures on which the FFT is applied are allocated using the \texttt{fftw\_malloc} subroutine, and deallocated using the \texttt{fftw\_free} subroutine, both of which are provided by the FFTW library. These subroutines have the same behaviour as the \texttt{allocate} and \texttt{deallocate} subroutines found in the Fortran standard library, however they also call \texttt{memalign} to ensure that data structures are properly aligned. Figure \ref{fig:memalign} shows contiguous aligned memory for a two-dimensional array in the Fortran programming language, which uses a row-major order for multidimensional array storage.
\par
Isca performs a transform from real to complex numbers and vice versa. A one-dimensional transform from a real array of size $N$ results in a complex array of size $N/2$. When implementing this using FFTW, the same input and output arrays are re-used for multiple transforms in order to take advantage of FFTWs plans. As \gls{fftw} computes an unnormalised \gls{dft}, the result is multiplied by the number of items in the input sequence. This means that the result must be scaled by a factor of $\frac{1}{N}$ after the \gls{dft} is performed, which adds a small overhead to compute costs in addition to the cost of the DFT.  

\subsubsection{Methodology}
To ensure that FFTW computes the same values as Temperton's FFT, both forward and backwards transforms were tested on sequences of known data. The results of this test shows that both transforms compute the exact same DFT, and IDFT for 30 unique sequences of data. Additionally, computing both the DFT and IDFT of a sequence in succession yields the original sequence upon which the transforms were applied.
\par
To compare the performance of FFTW against the original Temperton FFT found in Isca, the time taken to complete a number of one-dimensional FFTs was measured for both FFT implementations. Each FFT implementation was tested on four different sizes of randomly initialised two-dimensional data structures. For example, a two-dimensional array of size 128 $\times$ 64 will compute the DFT of 128 one-dimensional arrays of size 64. The sizes of the arrays tested correspond to the different array sizes used for the T42 (128 $\times$ 64), T85 (256 $\times$ 128) and T170 (512 $\times$ 256) model resolutions. To emulate the Isca code, the test program was compiled using the same compiler flags used to compile Isca. To negate the error incurred by fluctuations in compute costs caused by the random data used in each array, the mean value for 100 transformations was calculated.  \ref{fig:fft-times}.

\subsubsection{Results}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{img/compare_fft.pdf}
    \caption[Performance comparison of FFTW and Temperton's FFT]{The performance of Isca's bespoke Temperton FFT compared to the performance of FFTW across multiple domain sizes.  }
    \label{fig:fft-times}
\end{figure}
\par
The biggest performance improvement was observed on the Sandy Bridge processor when performing a FFT on a grid size representing the T170 resolution. For this configuration, the FFTW code ran 5.25$\times$ quicker than the Temperton FFT. Interestingly, a greater performance gain was measured when the FFT was applied to the largest data structure. why?
\par
Interestingly, the performance gain was only marginal for the T42 resolution when run on the ThunderX2 processor Interestingly, the T170 resolution gave the greatest performance boost, which suggests that the actual computation is much quicker... 
\par
Regardless of wether FFTW provides a significant improvement to Its better to use a library that is updated rather than relying on bespoke code that does not evolve with hardware. 
\par
Need to bear in mind that this test was performed on a single processor, so it is expected that the ThunderX2 will have the worst performance. 
\par
FFT only takes up around 5\% of the total compute costs, so overall not a massive win. But in terms of the actual FFT its massively quicker.
















\newpage
\section{Floating point precision}
On modern processors, the time taken to perform floating point arithmetic on a single data item is about the same for both single and double precision variables. At a larger scale, single precision arithmetic can improve performance by reducing memory bandwidth consumption, and by allowing for more data items to fit in a vector register. 
\par
Some of the most time consuming compute kernels in Isca operate over arrays of double precision complex numbers. In Fortran, a complex number is composed of a pair of floating point numbers, representing both real and imaginary parts of the complex number. This means that a double precision complex number of kind 8 uses 128 bits of memory; both real and imaginary parts of the number are a real value of 8 Bytes each. This means that vectorisation is costly in parts of the code that iterate over complex data structures, and impossible on the 128-bit registers used in Intels SSE, and Arms NEON SIMD extensions. Interestingly, Intel's cost model often determines that there is no benefit to using SIMD instructions on arrays of double precision complex numbers, even when 256-bit and 512-bit registers are available as a results of AVX2 and AVX512 respectively.
\par
To allow for SIMD instructions to be applied to complex data structures, Isca has been compiled to use single-precision floating point numbers. There is existing infrastructure to allow for this to be done as the codebase includes interfaces to both single and double precision versions of commonly used subroutines. To change the default memory usage of real and complex variables, a number of preprocessor directives and compiler flags can be specified at compile time. 

\subsection{Methodology}
To measure the performance difference between single and double precision numbers in Isca, both the Held-Suarez and Grey-Mars configurations were rerun using the single precision configuration. Additionally, the operational intensity (FLOPS/Byte) and performance (GFLOPS/s) were remeasured for the entire run of the executable to allow for comparison with the double precision code. 
\begin{itemize}
	\item Measure runtime of both sp and dp code
	\item Look at roofline, how has it changed? 
\end{itemize}

\subsection{Results}
For the single precision code, approximately 36\% of the program runtime was spent inside vectorised compute kernels, compared to just 28\% of runtime for the double precision code. This suggests that more loops are able to use vector instructions when using single precision variables. Vector reports produced when compiling the code confirm this, as vector instructions are now used to iterate over complex data structures on all processor architectures. 
\par
The roofline model in Figure \ref{}
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{img/roofline_model_bluepebble_precision.pdf}
\caption[Roofline model comparing single and double precision arithmetic]{Comparison of the single and double precision versions of Isca using the Held-Suarez and Grey-Mars configurations.}
\label{default}
\end{center}
\end{figure}
\par
Figure \ref{fig:precision} shows that the runtime is significantly reduced when using single precision arithmetic for all node types, and this result is consistent across both the Held-Suarez and Grey-Mars configurations. As the number of processor cores increases, the difference in runtime between the single and double precision code decreases. This is because of load balancing issues discussed in section x.
\par 
An additional benefit of using single precision arithmetic is that cache performance is improved and memory-bandwidth consumption is almost halved. 
 
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{img/single-double-precision.pdf}
\caption[Performance comparison of varied precision of floating point numbers]{Comparison of the wallclock runtimes for single and double precision floating point numbers for the Held-Suarez configuration running at T42 resolution. Single precision arithmetic is significantly faster at all core counts. }
\label{fig:precision}
\end{center}
\end{figure}
 
\subsection{Discussion}
In the Fortran 90 standard, single precision real numbers have 7 digits of accuracy, and double-precision real numbers have 15 digits of accuracy. Because of this, the units of last place numerical analysis technique cannot be used to compare single and double precision outputs. To verify the scientific validity of the results, both single and double precision output files were given to domain experts for comparison. 
\par
Although Isca is not stochastic, it is chaotic, which means that small changes to variables can lead to drastically different results. However, Isca is mainly used for the estimation of climate behaviour on exoplanets, and therefore outputs results as the averages of many numbers, so less precise variables are okay. 
\par
Single precision configurations could be applied when running a perturbed physics ensemble, which is a brute-force approach to model parameter selection. The process involves running many different simulations with a range of parameters. Single precision numbers could be used to find interesting parameter configurations, and promising results can be re-run at higher resolution using double precision.
\par
Look at the time taken to do loops. Plot another roofline model -> how has the oi changed? 
\par
Halves memory-bandwidth consumption. Therefore why isn't ThunderX2 performing better? 







\newpage
%%% MPI imbalance
\section{Load imbalance}
Isca exhibits a significant load-balancing issue whereby over 30\% of the program runtime is spent inside calls to \texttt{mpi\_recv}. It is not possible to improve upon this code due to the deep rooted imbalance throughput the code. This is just a necessary point of synchronisation that the code requires in order to operate. In order to make the code run faster it would require a complete re-write. Perhaps it may benefit from using some shared memory here, as 
\par
Graph of communication pattern
\par
Some special purposes require more expensive communication. For bit-reproducible global sums over 2D fields each processor gathers the pieces from all other processors to construct the global field. Examples for such expensive operations are surface flux corrections to maintain the total volume or salinity budget. How- ever, operations of this kind are rare in MOM-4 and can be avoided completely for regional models.
\par
Switched to an asynchronous call but this lead to segmentation faults. use some kind of graph to prove that there is a deeper rooted issue here.
\par
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{img/comm_mat_Held-Suarez.pdf}
\includegraphics[width=\textwidth]{img/comm_mat_grey-mars.pdf}
\caption[Communication matrices for Held-Suarez and Grey-Mars]{Communication matrices for both the Held-Suarez and Grey-mars model configurations when run at a resolution of T42. Communication time has been measured as the sum of all time spent inside the MPI library}
\label{fig:comm_mat}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
	\includegraphics[width=\textwidth]{img/mpi-barrier-time.pdf}
\caption[Comparison of the time spent in MPI]{Sum of total time spent at an MPI barrier for both the Held-Suarez and Grey-mars model configurations}
\label{fig:mpi-barrier}
\end{center}
\end{figure}


Slowest functions/subroutines as a result of MPI\_recv
\begin{description}
	\item[trans\_spherical\_to\_grid\_3d] Fields are transformed from spherical harmonics to a grid layout. Communication is required between all ranks to enable this to happen. Maybe we can do some rank reordering, or try to hide the cost of the comms between some compute? Will have to take a deeper look at the function.
	\item[area\_weighted\_global\_mean] Returns the area weighted global mean of a 2-d field input as a field of values local to the processor.
\end{description}

Further down the call stack, both of these issues are caused by \texttt{MPI\_wait}. This suggests that there is an imbalance in the MPI. Both these functions are something to do with the FFT, although the root cause of the problem is the MPI wait.

\clearpage
\subsection{Vector Instructions}
Intel advisor was run for both the Grey-Mars and Held-Suarez case. Running the Grey-Mars case, we find that there are five top time-consuming loops that are performing scalar operations, when vector operations may be able to be used. 
\par
Additionally, many of the vector instructions that are being used are only using SSE. Why are there not many AVX2/512 operations being performed? Can we force/coax the use of these wider registers to improve performance? 
\par
Inefficient memory access patterns may result in significant vector code execution slowdown or as in this case, block automatic vectorisation by the compiler.
\par
The top five slowest functions.

\par
At \texttt{spherical\_fourier.F90:312} and \texttt{spherical\_fourier.F90:315}, there is a similar problem due the nature of how the DFT to spherical harmonics transform works. The transform splits data into even and odd 
\par
To do this, it uses two fast Fourier transforms to convert between states. The first being \texttt{trans\_spherical\_to\_fourier}, and the second being \texttt{trans\_fourier\_to\_spherical}. This is the slowest part of the code, lasting for xxx\% of the runtime. 

\texttt{trans\_spherical\_to\_grid} and \texttt{trans\_grid\_to\_spherical} both call the \texttt{trans\_fourier\_to\_spherical} subroutines that take ages to compute. This means that it would be best to swap out \texttt{trans\_spherical\_to\_grid} for something else... 
\par
TODO: find a library that does what we want. Will need to check the outputs from both the FMS code, and the new library to make sure that they are the same. 
\par
There are some parallels between fftw3 and the implementation in Isca. fftw3 has two subroutines called \texttt{fftw\_plan\_many\_dft\_r2c} and \texttt{fftw\_plan\_many\_dft\_c2r}, that looks like they are used to perform many one-dimensional fft's over a two-dimensional array. Additionally, the \texttt{n} variable from \texttt{fft99} is used in the same way as \texttt{L} from fftw3 when allocating the memory for the complex type array in the Fourier domain (\texttt{n/2+1} or \texttt{L/2+1}). This means that these arrays are probably used in the same way. 
\par
To ensure that the results are the same, need to compare the results of the regular FFT to the results obtained using fftw. Sure that this will reveal the root of the problem. 
\par

Tested the results of the FFT by using a number of test scenarios using fft991 and fftw. The results were identical using both methods. 
\par


\begin{table}
\caption{TODO}
\begin{center}
\begin{tabular}{ c c c }
\toprule
File 					& Line 		& Performance Issue 					 \\\midrule
\texttt{fft99.F90}		& 1081		& Inefficient memory access patterns		  \\
\texttt{fft99.F90}		& 1049		& Inefficient memory access patterns		 \\
\bottomrule
\end{tabular}
\label{table:resolutions}
\end{center}
\end{table}





\section{Memory Leaks}
Memory leaks occur when allocated memory that is no longer needed is not released. Isca exhibits a large number of memory leaks whenever a new namelist is read in
\par
There are loads of memory leaks caused by incorrectly reading of namelists. There previous method was also incompatible with the Cray compiler -> maybe incorrect data was being read into the model previously? Was the model even correct? This code has been replaced at x locations throughout the code. Although there was no measurable performance benefit, this makes the code better.

\begin{lstlisting}[language=Fortran]
    read(input_nml_file, nml=transforms_nml, iostat=io)
    ierr = check_nml_error(io, `transforms_nml')
\end{lstlisting}

\begin{lstlisting}
    namelist_unit = open_namelist_file()
    read (namelist_unit, transforms_nml, iostat=io)
    close_file(namelist_unit)
    ierr = check_nml_error(io,'transforms_nml')
\end{lstlisting}





\section{Conclusions}
Although the FFT was improved, there is very little scope for optimisation without addressing the deeper rooted problem of the MPI imbalance.
\par
Moral of the story is that bespoke code does not age well. It is best to use a popular library that is regularly updated to support the newest hardware. 

%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=\textwidth]{img/opt_comparison.pdf}
%\caption{default}
%\label{default}
%\end{center}
%\end{figure}




%\section{Optimisation Methodology}
%Performance bottlenecks were found using the Intel Parallel Studio Extended Edition (PSXE) on \gls{bcp3}, \gls{bcp4} and \gls{bp}; Cray Performance Analysis and Tuning (CRAYPAT) was used on Isambard. 
%
%Intel splits their toolset into multiple products, each aiming to improve code performance through different means. The Intel PSXE manual recommends that the following tools are used to identify and address performance limiting factors of the following types.
%\begin{description}
%	\item[Advisor] This tool can be used to determine why some loops are not using vector instructions, as well as counting the FLOPs within time-consuming loops. 
%	\item[VTune Amplifier] CPU/ memory bound 
%	\item[Trace Analyzer and Collector] MPI bound 
%\end{description}
%
%\url{https://software.intel.com/sites/products/snapshots/application-snapshot/}.
%
%\subsection{Intel Tools}
%
%To find optimisations, the code was profiled using X, Y and Z. 




%%% Reflection and critical analysis

\part{Reflection and Critical Analysis}
\chapter{Reflection and Critical Analysis}
This project aimed to present 

\begin{itemize}
	\item It was difficult to find things to optimise. The biggest time-sink was the MPI, but that was due to unavoidable load-balancing issues that cannot be resolved through asynchronous message passing. 
	\item Prior to starting the project, there was no guarantee that the code could be optimised in any way. A lot of time was spent looking for stuff to optimise. 
	\item I spent a long time at the beginning of the project getting the code to work with the Cray compiler. This was maybe a waste of time. 
	\item I had never used Fortran before, and it was difficult to learn the language with such a massive code base. The codebase contains over 250,000 lines of code and a considerable amount of time was spent getting familiar with the different parts. 
	\item Couldn't do 3 runs for all test cases as there simply wasn't enough time. The model compiles into a special directory, and this means that only one model can be tested at one time. 
	\item Code crashes all the time because it's very delicate. 
	\item Although the Python library helped, it was still very difficult to coordinate all the testing as there were many experiments to do.
	\item Only benchmarked one code, therefore can't be a good reflection of the ThunderX2
	\item What do the results mean? 
	\item The codebase takes a long time to compile. The Cray compiler can take upwards of half an hour to compile the Grey-mars configuration. Sometimes a single code change can take half an hour to check if it works. 
\end{itemize}






% Bibliography
\bibliographystyle{ieeetr}
\bibliography{citations.bib}

\appendix
\chapter{Porting code changes}

\toclesssection{Isambard}
\subsection{Error 1}

\begin{lstlisting}
ftn-356 crayftn: ERROR MPP_DO_GLOBAL_FIELD2D_L8_3D, File = mpp_do_global_field.h, Line = 123, Column = 12   Assignment of a INTEGER expression to a LOGICAL variable is not allowed.
\end{lstlisting}
This issue was caused by a variation of the Fortran standard between the Cray, GNU and Intel compilers. GNU and Intel allow for the implicit conversion between logical, integer and real variables, but the Cray compiler does not. To resolve this issue, a new macro was defined to set a default value depending on the type of variables it was being used for. 
\par
If \texttt{MPP\_TYPE\_} is of type integer or real, then 
\begin{lstlisting}[language=Fortran]
#define MPP_DEFAULT_VALUE_ 0
\end{lstlisting}
\par
If \texttt{MPP\_TYPE\_} is of type complex, then 
\begin{lstlisting}
#define MPP_DEFAULT_VALUE_ .false.
\end{lstlisting}

\subsection{Error 2}

\begin{lstlisting}
ftn-1725 crayftn: ERROR COMPUTE_LC1, File = ../../../lustre/home/br-glancaster/Isca/src/atmos_spectral/init/polvani_2007.F90, Line = 356, Column = 26 
  Unexpected syntax while parsing the assignment statement : "operand" was expected but found "-".
\end{lstlisting}
Cray Fortran requires brackets around all values denoted as negative, for example
\begin{lstlisting}[language=Fortran]
Tr = T0 + lapse/(zt**-alpha + z(k)**-alpha)**(1./alpha)
\end{lstlisting}
becomes 
\begin{lstlisting}[language=Fortran]
Tr = T0 + lapse/(zt** (-alpha) + z(k)** (-alpha))**(1./alpha)
\end{lstlisting}




\chapter{Code listings}

\toclesssection{Grid to Fourier subroutine}

\begin{lstlisting}[language=Fortran,caption={Code used to perform an FFT using the FFTW library. This subroutine can be found in the new \texttt{fftw.F90} module, and transforms a 2D data structure from the spacial domain to frequency domain.}]
subroutine grid_to_fourier_double_2d_fftw(num, leng, lenc, grid, fourier)

integer(kind=4),            intent(in)    :: num    
integer(kind=4),            intent(in)    :: leng   
real(C_DOUBLE),             intent(in)    :: grid(leng, num)
complex(C_DOUBLE_COMPLEX),  intent(out)   :: fourier(lenc, num)
real                                      :: fact 
integer                                   :: i, j

fact = 1.0 / (leng - 1)

do j = 1, num
  do i = 1, leng - 1
    real_input(i) = grid(i,j)
  enddo

  call dfftw_execute_dft_r2c(real_input_pointer, real_input, complex_output)

  do i = 1, lenc
    fourier(i, j) = complex_output(i) * fact
  enddo
enddo
return
end subroutine grid_to_fourier_double_2d_fftw
\end{lstlisting}


\toclesssection{Program to time FFT}
\begin{lstlisting}[language=Fortran]
subroutine time_fft()
use fft_mod

real(kind=8)   , allocatable 	:: ain(:,:), aout(:,:)
complex(kind=8), allocatable 	:: four(:,:)
integer 	:: i, j, m, n, k, h, iter, lot
integer 	:: ntrans(3) = (/ 128, 256, 512 /)
integer 	:: lots(3) = (/ 64, 128, 256 /)
real 	    :: start_time = 0, stop_time = 0, mean_time_iter = 0, mean_time_full = 0, append_time = 0, time_3d_start = 0, time_3d_stop = 0

iter = 100

! test multiple transform lengths
  do m = 1, 3

  ! set up input data
    n = ntrans(m)
    lot = lots(m)

    allocate(ain(n+1,lot),aout(n+1,lot),four(n/2+1,lot))

    call fft_init(n)

    do k = 1, iter
        call random_number(ain(1:n,:))
        four = fft_grid_to_fourier(ain)
        call cpu_time(start_time)
            aout = fft_fourier_to_grid(four)
        call cpu_time(stop_time)
        append_time = append_time + (stop_time - start_time)
    enddo

    mean_time_iter = append_time / iter

    append_time = 0.0
    start_time = 0.0
    stop_time = 0.0

    do k = 1, iter
        call random_number(ain(1:n,:))
        four = fft_grid_to_fourier(ain)
        call cpu_time(time_3d_start)
        do h = 1, 25
            aout = fft_fourier_to_grid(four)
        enddo
        call cpu_time(time_3d_stop)
        append_time = append_time + (time_3d_stop - time_3d_start)
    enddo

    mean_time_full = append_time / iter

    call fft_end()
    deallocate (ain,aout,four)

    print *, '( ',n,' x ' ,lot ,' ), mean_iteration_time: '
    write (*,'(f15.9)') mean_time_iter
    print *, '( ',n,' x ' ,lot ,' ), mean_full_time: '
    write (*,'(f15.9)') mean_time_full
 enddo

end subroutine time_fft

end program test
\end{lstlisting}

\chapter{Job submission scripts}
\label{apx:submission}

\toclesssection{BCP3 (PBS)}
\begin{lstlisting}[language=bash]
#!/bin/sh

#PBS -n held_suarez_benchmarking
#PBS -V # export all environment variables to the batch job.
#PBS -d . # set working directory to .
#PBS -q long # submit to the long queue
#PBS -l nodes=1:ppn=16 
#PBS -l walltime=72:00:00 # Maximum wall time for the
#PBS -m e -M qv18258@bristol.ac.uk 

source activate isca_env
python $BENCHMARK_ISCA/src/main.py -codebase grey_mars -mincores 4 -maxcores 4 -r T21 -r T42 -r T85
\end{lstlisting}

\toclesssection{BluePebble (PBS Pro)}
\begin{lstlisting}[language=bash]
#!/bin/sh
#PBS -l select=1:ncpus=28:mem=20GB
#PBS -l walltime=72:00:00

module load tools/git/2.22.0
source activate isca_env
python $BENCHMARK_ISCA/src/main.py -mincores 16 -maxcores 16 -r T21 -r T42 -codebase grey_mars -fc kind_4
\end{lstlisting}



\toclesssection{Isambard (PBS Pro)}
\begin{lstlisting}[language=bash]
#!/bin/sh

#PBS -q arm
#PBS -l select=1:ncpus=64
#PBS -l walltime=23:00:00
#PBS -M =qv18258@bristol.ac.uk

source ~/isca_env/bin/activate
python $BENCHMARK_ISCA/src/main.py -mincores 4 -maxcores 4 -r T21 -r T42 -r T85 -codebase held_suarez -fc cray_temp
\end{lstlisting}

\toclesssection{BCP4 (SLURM)}

\begin{lstlisting}[language=bash]
#!/bin/bash

#SBATCH --job-name=benchmark_held_suarez_two_cores
#SBATCH --partition=cpu
#SBATCH --time=4-00:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=24
#number of cpus (cores) per task (process)
#SBATCH --cpus-per-task=1
#SBATCH --output=held_suarez_two_cores_%j.o
#SBATCH --mail-type=ALL
#SBATCH --mail-user=qv18258@bristol.ac.uk

echo Running on host `hostname`
echo Time is `date`
echo Directory is `pwd`

module purge
source $HOME/.bashrc
source $GFDL_BASE/src/extra/env/bristol-bc4
source activate isca_env

$HOME/.conda/envs/isca_env/bin/python $BENCHMARK_ISCA/src/main.py -mincores 2 -maxcores 2 -r T21 -r T42 -codebase held_suarez -fc gcc
\end{lstlisting}




\end{document}
